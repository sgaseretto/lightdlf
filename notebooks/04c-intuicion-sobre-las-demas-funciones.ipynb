{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuici칩n sobre el calculo de gradientes de las operaciones comunes sobre Tensores\n",
    "> Calculo de gradiente para otras funciones b치sicas entre tensores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el [notebook](04a-soporte-a-mas-funciones-con-autograd.ipynb) de autograd definimos diferentes operaciones comunes junto con el calculo de sus gradientes, en este notebook vemos de donde salen esos calculos de gradientes.\n",
    "\n",
    "Obs: La [notacion matem치tica](https://upmath.me/) para representar [operaciones entre tensores](http://www.malinc.se/math/latex/basiccodeen.php) puede no estar correcta. Otra gran ayuda para saber los signos en latex es [Detexify](http://detexify.kirelabs.org/classify.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Tensor(object):\n",
    "\n",
    "    def __init__(self, data,\n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        '''\n",
    "        Inicializa un tensor utilizando numpy\n",
    "\n",
    "        @data: una lista de numeros\n",
    "        @creators: lista de tensores que participarion en la creacion de un nuevo tensor\n",
    "        @creators_op: la operacion utilizada para combinar los tensores en el nuevo tensor\n",
    "        '''\n",
    "        self.data = np.array(data)\n",
    "        self.creation_op = creation_op\n",
    "        self.creators = creators\n",
    "        self.grad = None\n",
    "        self.autograd = autograd\n",
    "        self.children = {}\n",
    "        # se asigna un id al tensor\n",
    "        if (id is None):\n",
    "            id = np.random.randint(0, 100000)\n",
    "        self.id = id\n",
    "\n",
    "        # se hace un seguimiento de cuantos hijos tiene un tensor\n",
    "        # si los creadores no es none\n",
    "        if (creators is not None):\n",
    "            # para cada tensor padre\n",
    "            for c in creators:\n",
    "                # se verifica si el tensor padre posee el id del tensor hijo\n",
    "                # en caso de no estar, agrega el id del tensor hijo al tensor padre\n",
    "                if (self.id not in c.children):\n",
    "                    c.children[self.id] = 1\n",
    "                # si el tensor ya se encuentra entre los hijos del padre\n",
    "                # y vuelve a aparece, se incrementa en uno\n",
    "                # la cantidad de apariciones del tensor hijo\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "\n",
    "    def all_children_grads_accounted_for(self):\n",
    "        '''\n",
    "        Verifica si un tensor ha recibido la cantidad\n",
    "        correcta de gradientes por cada uno de sus hijos\n",
    "        '''\n",
    "        # print('tensor id:', self.id)\n",
    "        for id, cnt in self.children.items():\n",
    "            if (cnt != 0):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def backward(self, grad, grad_origin=None):\n",
    "        '''\n",
    "        Funcion que propaga recursivamente el gradiente a los creators o padres del tensor\n",
    "\n",
    "        @grad: gradiente\n",
    "        @grad_orign\n",
    "        '''\n",
    "        #         tab=tab\n",
    "        if (self.autograd):\n",
    "            if (grad_origin is not None):\n",
    "                # Verifica para asegurar si se puede hacer retropropagacion\n",
    "                if (self.children[grad_origin.id] == 0):\n",
    "                    raise Exception(\"No se puede retropropagar mas de una vez\")\n",
    "                # o si se est치 esperando un gradiente, en dicho caso se decrementa\n",
    "                else:\n",
    "                    # el contador para ese hijo\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "\n",
    "        # acumula el gradiente de multiples hijos\n",
    "        if (self.grad is None):\n",
    "            self.grad = grad\n",
    "        else:\n",
    "            self.grad += grad\n",
    "\n",
    "        if (self.creators is not None and\n",
    "                (self.all_children_grads_accounted_for() or grad_origin is None)):\n",
    "\n",
    "            if (self.creation_op == 'neg'):\n",
    "                self.creators[0].backward(self.grad.__neg__())\n",
    "                \n",
    "            if (self.creation_op == 'add'):\n",
    "                # al recibir self.grad, empieza a realizar backprop\n",
    "                self.creators[0].backward(self.grad, grad_origin=self)\n",
    "                self.creators[1].backward(self.grad, grad_origin=self)\n",
    "                \n",
    "            if(self.creation_op == \"sub\"):\n",
    "                self.creators[0].backward(Tensor(self.grad.data), self)\n",
    "                self.creators[1].backward(Tensor(self.grad.__neg__().data), self)\n",
    "\n",
    "            if(self.creation_op == \"mul\"):\n",
    "                new = self.grad * self.creators[1]\n",
    "                self.creators[0].backward(new , self)\n",
    "                new = self.grad * self.creators[0]\n",
    "                self.creators[1].backward(new, self)                    \n",
    "\n",
    "            if(self.creation_op == \"mm\"):\n",
    "                layer = self.creators[0]                    # activaciones => layer\n",
    "                weights = self.creators[1]                  # pesos = weights\n",
    "                # c0 = self.creators[0]                       # activaciones => layer\n",
    "                # c1 = self.creators[1]                       # pesos = weights\n",
    "                # new = self.grad.mm(c1.transpose())  # grad = delta => delta x weights.T\n",
    "                new = Tensor.mm(self.grad, weights.transpose())  # grad = delta => delta x weights.T\n",
    "                layer.backward(new)\n",
    "                # c0.backward(new)                            \n",
    "                # new = self.grad.transpose().mm(c0).transpose() # (delta.T x layer).T = layer.T x delta\n",
    "                new = Tensor.mm(layer.transpose(), self.grad)  # layer.T x delta\n",
    "                weights.backward(new)\n",
    "                # c1.backward(new)\n",
    "\n",
    "            if(self.creation_op == \"transpose\"):\n",
    "                self.creators[0].backward(self.grad.transpose())\n",
    "\n",
    "            if(\"sum\" in self.creation_op):\n",
    "                dim = int(self.creation_op.split(\"_\")[1])\n",
    "                self.creators[0].backward(self.grad.expand(dim, self.creators[0].data.shape[dim]))\n",
    "\n",
    "            if(\"expand\" in self.creation_op):\n",
    "                dim = int(self.creation_op.split(\"_\")[1])\n",
    "                self.creators[0].backward(self.grad.sum(dim))\n",
    "                \n",
    "                \n",
    "    def __neg__(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data * -1,\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op='neg')\n",
    "        return Tensor(self.data * -1)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        '''\n",
    "        @other: un Tensor\n",
    "        '''\n",
    "        if (self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self, other],\n",
    "                          creation_op='add')\n",
    "        return Tensor(self.data + other.data)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        '''\n",
    "        @other: un Tensor\n",
    "        '''\n",
    "        if (self.autograd and other.autograd):\n",
    "            return Tensor(self.data - other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self, other],\n",
    "                          creation_op='sub')\n",
    "        return Tensor(self.data - other.data)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        '''\n",
    "        @other: un Tensor\n",
    "        '''\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data * other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"mul\")\n",
    "        return Tensor(self.data * other.data)\n",
    "    \n",
    "    def sum(self, dim):\n",
    "        '''\n",
    "        Suma atravez de dimensiones, si tenemos una matriz 2x3 y \n",
    "        aplicamos sum(0) sumara todos los valores de las filas \n",
    "        dando como resultado un vector 1x3, en cambio si se aplica\n",
    "        sum(1) el resultado es un vector 2x1\n",
    "        \n",
    "        @dim: dimension para la suma\n",
    "        '''\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.sum(dim),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sum_\"+str(dim))\n",
    "        return Tensor(self.data.sum(dim))\n",
    "    \n",
    "    def expand(self, dim, copies):\n",
    "        '''\n",
    "        Se utiliza para retropropagar a traves de una suma sum().\n",
    "        Copia datos a lo largo de una dimension\n",
    "        '''\n",
    "\n",
    "        trans_cmd = list(range(0,len(self.data.shape)))\n",
    "        trans_cmd.insert(dim,len(self.data.shape))\n",
    "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)\n",
    "        \n",
    "        if(self.autograd):\n",
    "            return Tensor(new_data,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"expand_\"+str(dim))\n",
    "        return Tensor(new_data)\n",
    "    \n",
    "    def transpose(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.transpose(),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"transpose\")\n",
    "        \n",
    "        return Tensor(self.data.transpose())\n",
    "    \n",
    "    def mm(self, x):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.dot(x.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self,x],\n",
    "                          creation_op=\"mm\")\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "    \n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def __sub__(self, other):\n",
    "'''\n",
    "@other: un Tensor\n",
    "'''\n",
    "if (self.autograd and other.autograd):\n",
    "    return Tensor(self.data - other.data,\n",
    "                  autograd=True,\n",
    "                  creators=[self, other],\n",
    "                  creation_op='sub')\n",
    "return Tensor(self.data - other.data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_grads(tensor: Tensor):\n",
    "    for c in tensor.creators:\n",
    "        c.grad.data *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = Tensor([3,3,3], autograd=True)\n",
    "x2 = Tensor([2,2,2], autograd=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resta o ```sub```\n",
    "Calculamos la resta entre los tensores x1 y x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x1-x2\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora queremos calcular el gradiente que deber치 ser pasado tanto a x1 como a x2 a partir de esta operacion. Sabemos que:\n",
    "- x1 es el ```minuendo```\n",
    "- x2 es el ```sustraendo```\n",
    "\n",
    "Esto se puede verificar facilmente mirando quienes fueron los creadores de ```y```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(y.creators[0].id == x1.id)\n",
    "print(y.creators[1].id == x2.id)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de una resta, el gradiente que debemos pasar a los creadores del tensor y debe ser:\n",
    "- Para el minuendo, el gradiente del hijo\n",
    "- Para el sustraendo, el la negacion ( todo por -1) del gradiente del hijo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: 42962 \n",
      " [1 1 1]\n",
      "x2:  98957 \n",
      " [-1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "y_grad = Tensor(np.ones_like(y.data))\n",
    "x1.grad = y_grad\n",
    "x2.grad = y_grad.__neg__()\n",
    "print('x1:',x1.id,'\\n', x1.grad)\n",
    "print('x2: ',x2.id,'\\n', x2.grad)\n",
    "zero_grads(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementando esto dentro de la funcion ```backward()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(Tensor(np.ones_like(y.data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos que esto se aplique correctamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.creators[0]:  42962 \n",
      " [1 1 1]\n",
      "y.creators[1]:  98957 \n",
      " [-1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "print('y.creators[0]: ',y.creators[0].id,'\\n', y.creators[0].grad)\n",
    "print('y.creators[1]: ',y.creators[1].id,'\\n', y.creators[1].grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiplicacion elemento por elemento o ```mul```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 6, 6])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x1*x2\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora queremos calcular el gradiente que deber치 ser pasado tanto a $x_1$ como a $x_2$ a partir de esta operacion.\n",
    "\n",
    "Como los gradientes se optienen a partir de ```derivadas parciales``` debemos calcular la derivada parcial de $x_1*x_2$ con respecto a $x_1$ y con respecto a $x_2$ por el gradiente que viene del tensor hijo ($y$) y pasarle este nuevo valor a los tensores padres como gradientes\n",
    "A modo de ejemplo consideremos solo $x_1$:\n",
    "$$f(x_1,x_2) = x_1 * x_2$$\n",
    "Al calcular la derivada parcial con respecto a $x_1$, $x_2$ queda como constante, por tanto:\n",
    "$${d f(x_1,x_2)\\over dx_1} = x_2 $$\n",
    "A todo esto debemos multiplicarle el gradiente que viene del tensor hijo por lo que queda de la siguiente manera:\n",
    "\n",
    "$$grad_x = grad_y * x_2$$\n",
    "\n",
    "Como el gradiente debe ser pasado a ambos tensores, para el caso de $x_2$ quedar칤a de la siguiente manera:\n",
    "$$f(x_1,x_2) = x_1 * x_2$$\n",
    "$${d f(x_1,x_2)\\over dx_2} = x_1 $$\n",
    "A todo esto debemos multiplicarle el gradiente que viene del tensor hijo por lo que queda de la siguiente manera:\n",
    "$$grad_x = grad_y * x_1$$\n",
    "\n",
    "Por tanto, para realizar retropropagaci칩n de una multiplicacion elemento a elemento, debemos crear un metodo que pase estos gradientes a los tensores padres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_grad = Tensor(np.ones_like(y.data))\n",
    "x1.grad = y_grad * x2\n",
    "x2.grad = y_grad * x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: 42962 \n",
      " [2 2 2]\n",
      "x2:  98957 \n",
      " [3 3 3]\n"
     ]
    }
   ],
   "source": [
    "print('x1:',x1.id,'\\n', x1.grad)\n",
    "print('x2: ',x2.id,'\\n', x2.grad)\n",
    "zero_grads(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementando esto dentro de la funcion ```backward()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(Tensor(np.ones_like(y.data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.creators[0]:  42962 \n",
      " [2 2 2]\n",
      "y.creators[1]:  98957 \n",
      " [3 3 3]\n"
     ]
    }
   ],
   "source": [
    "print('y.creators[0]: ',y.creators[0].id,'\\n', y.creators[0].grad)\n",
    "print('y.creators[1]: ',y.creators[1].id,'\\n', y.creators[1].grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiplicacion de Matrices o ```mm```\n",
    "Esta operacion es clave a la hora de multiplicar entradas o ```칣nputs``` con los pesos de una capa de una red neuronal y permite obtener la salida de una capa. Se trata de realizar filas por columnas.\n",
    "\n",
    "Consideremos como ejemplo un tensor $x_1$ de dimensiones $(2,2)$ y un tensor $x_2$ de dimensiones $(2,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = Tensor([[2,2,2],\n",
    "             [2,2,2]], autograd=True)\n",
    "\n",
    "x2 = Tensor([[3],\n",
    "             [3],\n",
    "             [3]], autograd=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regla para multiplicar matrices entre s칤 es que el numero de filas de la primera matriz debe ser igual a la segunda y como resultado se obtiene una matriz con el mismo numero de filas que la primera columna y el mismo numero de columnas que la segunda matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1)\n",
      "[[18]\n",
      " [18]]\n"
     ]
    }
   ],
   "source": [
    "y = Tensor.mm(x1,x2)\n",
    "print(y.data.shape)\n",
    "print(y.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_grad = Tensor(np.ones_like(y.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entender de una forma m치s intuitiva la multiplicacion de matrices podemos observar un ejemplo en el siguiente [link](http://matrixmultiplication.xyz/) (reemplazar los valores de la matriz por los mismos de este notebook).\n",
    "Ahora es necesario pasar el gradiente desde $Y$ hasta sus tensores ```padres``` $X_1$ y $X_2$. Tenemos los siguientes tensores:\n",
    "\n",
    "$$X_1 = \\begin{pmatrix}\n",
    "2& 2& 2\\\\\n",
    "2& 2& 2\\\\\n",
    "\\end{pmatrix},$$\n",
    "\n",
    "$$X_2 = \\begin{pmatrix}\n",
    "3\\\\\n",
    "3\\\\\n",
    "3\\\\\n",
    "\\end{pmatrix},$$\n",
    "\n",
    "$$grad_Y = \\begin{pmatrix}\n",
    "1\\\\\n",
    "1\\\\\n",
    "\\end{pmatrix},$$\n",
    "\n",
    "Como el gradiente debe ser pasado a los dos tensores que generaron $Y$, es necesario generar, utilizando el gradiente, tensores con las mismas dimensiones para cada tensor \"padre\", es decir:\n",
    "- Si $X_1$ es un tensor $(2,3)$, debemos pasar como gradiente un tensor con dimension $(2,3)$\n",
    "- Si $X_2$ es un tensor $(3,1)$, debemos pasar como gradiente un tensor con dimension $(3,1)$\n",
    "- Sabemos tambien que tanto $y$ como $y_{grad}$ son de dimensiones $(2,1)$\n",
    "\n",
    "En el caso de $X_1$ podemos regenerar un tensor $(2,3)$ utilizando ${grad_Y}$ y la transpuesta de $X_2$\n",
    "\n",
    "$$grad_{X_1} = grad_Y * {X_2^T} = \n",
    "\\begin{pmatrix}\n",
    "1\\\\\n",
    "1\\\\\n",
    "\\end{pmatrix} \\times\n",
    "\\begin{pmatrix}\n",
    "3& 3& 3\\\\\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "3& 3& 3\\\\\n",
    "3& 3& 3\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "En el caso de $X_2$ podemos regenerar un tensor $(3,1)$ utilizando la transpuesta de $X_1$ y ${grad_Y}$\n",
    "\n",
    "$$grad_{X_1} = grad_Y * {X_2^T} = \n",
    "\\begin{pmatrix}\n",
    "2& 2\\\\\n",
    "2& 2\\\\\n",
    "2& 2\\\\\n",
    "\\end{pmatrix} \\times\n",
    "\\begin{pmatrix}\n",
    "1\\\\\n",
    "1\\\\\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "4\\\\\n",
    "4\\\\\n",
    "4\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Podemos decir que b치sicamente el gradiente de una multiplicacion de matrices es el producto de la multiplicacion de matrices por la transpuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1.grad = y_grad.mm(x2.transpose())\n",
    "x2.grad = x1.transpose().mm(y_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: 41967 \n",
      " [[3 3 3]\n",
      " [3 3 3]]\n",
      "x2:  26556 \n",
      " [[4]\n",
      " [4]\n",
      " [4]]\n"
     ]
    }
   ],
   "source": [
    "print('x1:',x1.id,'\\n', x1.grad)\n",
    "print('x2: ',x2.id,'\\n', x2.grad)\n",
    "zero_grads(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(Tensor(np.ones_like(y.data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.creators[0]:  41967 \n",
      " [[3 3 3]\n",
      " [3 3 3]]\n",
      "y.creators[1]:  26556 \n",
      " [[4]\n",
      " [4]\n",
      " [4]]\n"
     ]
    }
   ],
   "source": [
    "print('y.creators[0]: ',y.creators[0].id,'\\n', y.creators[0].grad)\n",
    "print('y.creators[1]: ',y.creators[1].id,'\\n', y.creators[1].grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transpuesta de Matrices o ```transpose```\n",
    "Hacer backward de la funcion transpuesta es sencillo\n",
    "\n",
    "- Si tenemos una matriz $X$ de dimensiones $(a,b)$ la transpuesta $Y=X'$ tendr치 las dimensiones $(b,a)$\n",
    "- Como tenemos un grafo computacional, debemos pasar el gradiente de $Y$ al tensor padre, en este caso $X$\n",
    "- Como el gradiente $grad_Y$ en $Y$ es de dimension $(b,a)$ al pasar este gradiente a su tensor padre, debemos pasarlo de manera que tenga la misma forma que el tensor padre $X$, por tanto aplicando nuevamente la transpuesta a $grad_Y$, tenemos un tensor con las mismas dimensiones que $X$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Tensor([[2,2,2],\n",
    "             [2,2,2]], autograd=True)\n",
    "x.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = Tensor.transpose(x)\n",
    "y.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  44681 \n",
      " (2, 3) \n",
      " [[1 1 1]\n",
      " [1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "y_grad = Tensor(np.ones_like(y.data))\n",
    "y_grad.data.shape\n",
    "x.grad = y_grad.transpose()\n",
    "print('x: ',x.id,'\\n', x.grad.data.shape, '\\n', x.grad.data)\n",
    "zero_grads(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward(np.ones_like(y.data))\n",
    "y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1],\n",
       "       [1, 1, 1]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suma y Expancion\n",
    "Estas operaciones siguen una l칩gica similar entre s칤. Se utilizan para sumar las filas o columnas de una matriz, o para expandir n veces (copiar) las columnas de un vector columna o expandir n veces las filas de un vector fila\n",
    "\n",
    "Partamos de un tensor $X$ de dimension $(2,3)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Tensor([[2,2,2],\n",
    "             [2,2,2]], autograd=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En ```numpy``` y por consiguiente en ```lightdlf``` se guardan las dimensiones en atributo llamado ```shape``` el cual es una ```tupla```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo que si queremos saber cuantas filas o cuantas columnas tiene el tensor debemos seguir el orden en el que aparecen en la tupla ```shape```, el valor en el ```indice 0``` se correspondera a la cantidad de filas y el valor en el ```indice 1``` al numero de columnas. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filas: 2\n",
      "columntas: 3\n"
     ]
    }
   ],
   "source": [
    "print('filas:', x.data.shape[0])\n",
    "print('columntas:', x.data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos sumar todas las filas entre s칤, es decir tomar todos los elementos de una columna y sumarlos para obtener un solo valor por columna, obtendremos un vector fila. Para referenciar a las filas debemos usar el ```indice 0``` de la tupla ```shape``` para que la funcion sepa que queremos sumar las filasA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_0 = x.sum(0)\n",
    "x_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo mismo si queremos sumar las columnas, con la diferencia de que ahora queremos sumar las columnas, es decir, sumar todos los valores de una fila en un solo valor. En este caso el indice de la tupla ```shape ``` que determina las columnas es el ```indice 1```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 6])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1 = x.sum(1)\n",
    "x_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora digamos que queremos pasar gradientes a $X$ desde $X_0$ o $X_1$, para esto usamos la operacion de expanci칩n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 4, 4],\n",
       "       [4, 4, 4]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_0 = x_0.expand(dim=0,copies=x.data.shape[0])\n",
    "y_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6, 6, 6],\n",
       "       [6, 6, 6]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_1 = x_1.expand(dim=1, copies=x.data.shape[1])\n",
    "y_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso inverso, en el que partamos de un vector columna o un vector fila, y lo expandimos a una matriz, para pasar el gradiente al vector padre debemos realizar una suma, usando el eje 0 en caso de ser un vector fila y el eje 1 en caso de ser un vector columna"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
