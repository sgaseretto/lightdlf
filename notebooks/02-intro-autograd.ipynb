{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduccion al Cálculo Automático de Gradiente\n",
    "En lugar de calcular manualmente el gradiente de cada capa obteniendo la derivada del resultado de la capa anterior, los frameworks de deeplearning hacen esto de forma automática. Para poder agregar esta funcionalidad, debemos agregar a los tensores la capacidad de crear algo llamado ```Grafo computacional```\n",
    "## Grafo Computacional o ```computational grapg```\n",
    "Para poder realizar ```retropropagación``` o ```backprop``` sobre una red neuronal, debemos ser capaces de poder rastrear y determinar todas las operaciones y transformaciones que son realizadas sobre los tensores que componen la arquitectura de la red. Para esto definimos lo que se llama un grafo computacional. Que básicamente, si obtuvimos un tensor ```z``` a partir de la suma de los tensores ```x``` e ```y```, luego de obtener el valor de z y el error, debemos poder propagar el error por el resto de los tensores que participaron en la creacion de ```z``` es decir por ```x``` e ```y``` y teniendo en cuenta la operación mediante la cual se combinaron.\n",
    "\n",
    "Para esto debemos agregar a la clase ```Tensor``` los siguientes atributos:\n",
    "- ```creators```: que es una lista de tensores que originaron el nuevo tensor y\n",
    "- ```creation_op```: que permite saber qué operacion utilizaron los ```creators``` para combinarse y crear el nuevo tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor(object):\n",
    "    \n",
    "    def __init__(self, data, creators=None, creation_op=None):\n",
    "        '''\n",
    "        Inicializa un tensor utilizando numpy\n",
    "        \n",
    "        @data: una lista de numeros\n",
    "        @creators: lista de tensores que participarion en la creacion de un nuevo tensor\n",
    "        @creators_op: la operacion utilizada para combinar los tensores en el nuevo tensor\n",
    "        '''\n",
    "        self.data = np.array(data)\n",
    "        self.creation_op = creation_op\n",
    "        self.creators = creators\n",
    "        self.grad = None\n",
    "        \n",
    "    def backward(self, grad):\n",
    "        '''\n",
    "        Funcion que propaga recursivamente el gradiente a los creators del tensor\n",
    "        \n",
    "        @grad: gradiente \n",
    "        '''\n",
    "        self.grad = grad\n",
    "        \n",
    "        if (self.creation_op == 'add'):\n",
    "            self.creators[0].backward(grad)\n",
    "            self.creators[1].backward(grad)\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        '''\n",
    "        @other: un Tensor\n",
    "        '''\n",
    "        return Tensor(self.data + other.data,\n",
    "                      creators=[self, other],\n",
    "                      creation_op='add')\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = Tensor([1,1,1,1,1])\n",
    "x = Tensor([1,2,3,4,5])\n",
    "y = Tensor([2,2,2,2,2])\n",
    "z = x + y\n",
    "z.backward(one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grafo computacional de la suma anterior se podría visualizar de la siguiente manera\n",
    "\n",
    "<img src=\"./diagrams/01-computational-graph-add.png\" class=\"center\">\n",
    "\n",
    "Donde los ```creators``` fueron ```[x, y]``` y se combinaron utilizando el ```creations_op``` de ```add``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradientes\n",
      "z [1 1 1 1 1]\n",
      "x [1 1 1 1 1]\n",
      "y [1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print('gradientes')\n",
    "print('z', z.grad)\n",
    "print('x', x.grad)\n",
    "print('y', y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como el gradiente de una suma es la suma, podemos ver que los gradientes para los creators es el valor al cual hicimoz ```backward```, en este caso ```[1,1,1,1,1]```\n",
    "\n",
    "Como ```backward``` propaga el gradiente de forma recursiva, veamos que ocurre si creamos un grafo computacional más profundo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor([2,2,2,2])\n",
    "b = Tensor([3,3,3,3])\n",
    "c = Tensor([4,4,4,4])\n",
    "d = a + b\n",
    "e = c + d\n",
    "e.backward([1,1,1,1])\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple vista la clase ```Tensor``` parece construir correctamente el grafo computacional, pero que ocurre si un tensor es utilizado como creator para mas de un tensor? algo como la siguiente imagen donde el tensor ```b``` es el creator de otros dos tensores:\n",
    "\n",
    "<img src=\"./diagrams/02-computational-graph-add.png\" class=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor([1,2,3,4,5])\n",
    "b = Tensor([2,2,2,2,2])\n",
    "c = Tensor([5,4,3,2,1])\n",
    "d = a + b\n",
    "e = b + c\n",
    "f = d + e\n",
    "f.backward(Tensor([1,1,1,1,1]))\n",
    "print(b.grad.data == np.array([2,2,2,2,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El gradiente ```[1,1,1,1,1]``` es pasado a ```b``` dos veces, por lo que dicho gradiente se debería sumar y ser ```[2,2,2,2,2]``` pero en lugar de eso sigue siendo ```[1,1,1,1,1]```. En el siguiente [notebook](./03-tensores-de-usos-multiples.ipynb) se agregará soporte a la clase ```Tensor``` para solucionar este problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
