{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agregando soporte a más funciones a autograd para retropropagar\n",
    "En el [notebook](./03-tensores-de-usos-multiples.ipynb) anterior se implemento autograd con la operación de ```suma``` pero en un grafo computacional la suma es una de muchas otras operaciones que se pueden realizar con tensores. Por tanto es necesario definir estas operaciones con sus respectivos gradientes o derivadas.\n",
    "\n",
    "Para la suma definimos el siguiente metodo:\n",
    "```\n",
    "def __add__(self, other):\n",
    "    if(self.autograd and other.autograd):\n",
    "        return Tensor(self.data + other.data,\n",
    "                        autograd=True,\n",
    "                        creators=[self,other],\n",
    "                        creation_op=\"add\")\n",
    "return Tensor(self.data + other.data)\n",
    "```\n",
    "Y para la retropropagacion definimos la siguiente condicion dentro del método ```backward()```:\n",
    "```\n",
    "if (self.creation_op == 'add'):\n",
    "    self.creators[0].backward(self.grad, grad_origin=self)\n",
    "    self.creators[1].backward(self.grad, grad_origin=self)\n",
    "```\n",
    "Aparte de estas dos secciones de código, la retropropagación no se maneja en ninguna otra parte de la clase ```Tensor```. Toda la lógica de la retropropagación se abstrae para que todo lo necesario para la suma sea definido solamente en esas dos partes.\n",
    "\n",
    "Hay que notar también de que la retropropagación o ```backprop``` (a partir de ahora) no se realiza en el caso de que el tensor tenga el atributo ```autograd = False```, por eso la primera linea de ```__add__``` contiene una condicion que verifica que los dos tensores participando de la suma tengan ```self.autograd == True``` para que al instanciar un nuevo tensor, este también tenga ```self.autograd == True```\n",
    "\n",
    "## Añadamos la soporte para la ```negacion```\n",
    "\n",
    "La funcion de negacion se logra agregando las siguientes secciones de código a la clase:\n",
    "Para realizar la ```negacion```:\n",
    "```\n",
    "def __neg__(self):\n",
    "    if(self.autograd):\n",
    "        return Tensor(self.data * -1,\n",
    "                     autograd=True,\n",
    "                     creators=[self],\n",
    "                     creation_op='neg')\n",
    "    return Tensor(self.data * -1)\n",
    "```\n",
    "Y dentro de la funcion ```backward()```\n",
    "```\n",
    "if (self.creation_op == 'neg'):\n",
    "    self.creators[0].backward(self.grad.__neg__())\n",
    "```\n",
    "Como la negacion involucra solo a un tensor, se hace backward solo para un solo ```creator```. Al retropropagar el gradiente de un tensor \"negado\", el signo de este gradiente también se invierte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Tensor(object):\n",
    "\n",
    "    def __init__(self, data,\n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        '''\n",
    "        Inicializa un tensor utilizando numpy\n",
    "\n",
    "        @data: una lista de numeros\n",
    "        @creators: lista de tensores que participarion en la creacion de un nuevo tensor\n",
    "        @creators_op: la operacion utilizada para combinar los tensores en el nuevo tensor\n",
    "        '''\n",
    "        self.data = np.array(data)\n",
    "        self.creation_op = creation_op\n",
    "        self.creators = creators\n",
    "        self.grad = None\n",
    "        self.autograd = autograd\n",
    "        self.children = {}\n",
    "        # se asigna un id al tensor\n",
    "        if (id is None):\n",
    "            id = np.random.randint(0, 100000)\n",
    "        self.id = id\n",
    "\n",
    "        # se hace un seguimiento de cuantos hijos tiene un tensor\n",
    "        # si los creadores no es none\n",
    "        if (creators is not None):\n",
    "            # para cada tensor padre\n",
    "            for c in creators:\n",
    "                # se verifica si el tensor padre posee el id del tensor hijo\n",
    "                # en caso de no estar, agrega el id del tensor hijo al tensor padre\n",
    "                if (self.id not in c.children):\n",
    "                    c.children[self.id] = 1\n",
    "                # si el tensor ya se encuentra entre los hijos del padre\n",
    "                # y vuelve a aparece, se incrementa en uno\n",
    "                # la cantidad de apariciones del tensor hijo\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "\n",
    "    def all_children_grads_accounted_for(self):\n",
    "        '''\n",
    "        Verifica si un tensor ha recibido la cantidad\n",
    "        correcta de gradientes por cada uno de sus hijos\n",
    "        '''\n",
    "        # print('tensor id:', self.id)\n",
    "        for id, cnt in self.children.items():\n",
    "            if (cnt != 0):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def backward(self, grad, grad_origin=None):\n",
    "        '''\n",
    "        Funcion que propaga recursivamente el gradiente a los creators o padres del tensor\n",
    "\n",
    "        @grad: gradiente\n",
    "        @grad_orign\n",
    "        '''\n",
    "        #         tab=tab\n",
    "        if (self.autograd):\n",
    "            if (grad_origin is not None):\n",
    "                # Verifica para asegurar si se puede hacer retropropagacion\n",
    "                if (self.children[grad_origin.id] == 0):\n",
    "                    raise Exception(\"No se puede retropropagar mas de una vez\")\n",
    "                # o si se está esperando un gradiente, en dicho caso se decrementa\n",
    "                else:\n",
    "                    # el contador para ese hijo\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "\n",
    "        # acumula el gradiente de multiples hijos\n",
    "        if (self.grad is None):\n",
    "            self.grad = grad\n",
    "        else:\n",
    "            self.grad += grad\n",
    "\n",
    "        if (self.creators is not None and\n",
    "                (self.all_children_grads_accounted_for() or grad_origin is None)):\n",
    "\n",
    "            if (self.creation_op == 'add'):\n",
    "                # al recibir self.grad, empieza a realizar backprop\n",
    "                self.creators[0].backward(self.grad, grad_origin=self)\n",
    "                self.creators[1].backward(self.grad, grad_origin=self)\n",
    "                \n",
    "            if (self.creation_op == 'neg'):\n",
    "                self.creators[0].backward(self.grad.__neg__())\n",
    "\n",
    "    def __add__(self, other):\n",
    "        '''\n",
    "        @other: un Tensor\n",
    "        '''\n",
    "        if (self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self, other],\n",
    "                          creation_op='add')\n",
    "        return Tensor(self.data + other.data)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data * -1,\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op='neg')\n",
    "        return Tensor(self.data * -1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "x = Tensor([1,1,1,1], autograd=True)\n",
    "y = (-x) + (-x)\n",
    "y.backward(Tensor([1,1,1,1]))\n",
    "print(x.grad.data == np.array([-2,-2,-2,-2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agregando soporte a más funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Tensor(object):\n",
    "\n",
    "    def __init__(self, data,\n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        '''\n",
    "        Inicializa un tensor utilizando numpy\n",
    "\n",
    "        @data: una lista de numeros\n",
    "        @creators: lista de tensores que participarion en la creacion de un nuevo tensor\n",
    "        @creators_op: la operacion utilizada para combinar los tensores en el nuevo tensor\n",
    "        '''\n",
    "        self.data = np.array(data)\n",
    "        self.creation_op = creation_op\n",
    "        self.creators = creators\n",
    "        self.grad = None\n",
    "        self.autograd = autograd\n",
    "        self.children = {}\n",
    "        # se asigna un id al tensor\n",
    "        if (id is None):\n",
    "            id = np.random.randint(0, 100000)\n",
    "        self.id = id\n",
    "\n",
    "        # se hace un seguimiento de cuantos hijos tiene un tensor\n",
    "        # si los creadores no es none\n",
    "        if (creators is not None):\n",
    "            # para cada tensor padre\n",
    "            for c in creators:\n",
    "                # se verifica si el tensor padre posee el id del tensor hijo\n",
    "                # en caso de no estar, agrega el id del tensor hijo al tensor padre\n",
    "                if (self.id not in c.children):\n",
    "                    c.children[self.id] = 1\n",
    "                # si el tensor ya se encuentra entre los hijos del padre\n",
    "                # y vuelve a aparece, se incrementa en uno\n",
    "                # la cantidad de apariciones del tensor hijo\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "\n",
    "    def all_children_grads_accounted_for(self):\n",
    "        '''\n",
    "        Verifica si un tensor ha recibido la cantidad\n",
    "        correcta de gradientes por cada uno de sus hijos\n",
    "        '''\n",
    "        # print('tensor id:', self.id)\n",
    "        for id, cnt in self.children.items():\n",
    "            if (cnt != 0):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def backward(self, grad, grad_origin=None):\n",
    "        '''\n",
    "        Funcion que propaga recursivamente el gradiente a los creators o padres del tensor\n",
    "\n",
    "        @grad: gradiente\n",
    "        @grad_orign\n",
    "        '''\n",
    "        #         tab=tab\n",
    "        if (self.autograd):\n",
    "            if (grad_origin is not None):\n",
    "                # Verifica para asegurar si se puede hacer retropropagacion\n",
    "                if (self.children[grad_origin.id] == 0):\n",
    "                    raise Exception(\"No se puede retropropagar mas de una vez\")\n",
    "                # o si se está esperando un gradiente, en dicho caso se decrementa\n",
    "                else:\n",
    "                    # el contador para ese hijo\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "\n",
    "        # acumula el gradiente de multiples hijos\n",
    "        if (self.grad is None):\n",
    "            self.grad = grad\n",
    "        else:\n",
    "            self.grad += grad\n",
    "\n",
    "        if (self.creators is not None and\n",
    "                (self.all_children_grads_accounted_for() or grad_origin is None)):\n",
    "\n",
    "            if (self.creation_op == 'neg'):\n",
    "                self.creators[0].backward(self.grad.__neg__())\n",
    "                \n",
    "            if (self.creation_op == 'add'):\n",
    "                # al recibir self.grad, empieza a realizar backprop\n",
    "                self.creators[0].backward(self.grad, grad_origin=self)\n",
    "                self.creators[1].backward(self.grad, grad_origin=self)\n",
    "                \n",
    "            if(self.creation_op == \"sub\"):\n",
    "                self.creators[0].backward(Tensor(self.grad.data), self)\n",
    "                self.creators[1].backward(Tensor(self.grad.__neg__().data), self)\n",
    "\n",
    "            if(self.creation_op == \"mul\"):\n",
    "                new = self.grad * self.creators[1]\n",
    "                self.creators[0].backward(new , self)\n",
    "                new = self.grad * self.creators[0]\n",
    "                self.creators[1].backward(new, self)                    \n",
    "\n",
    "            if(self.creation_op == \"mm\"):\n",
    "                layer = self.creators[0]                    # activaciones => layer\n",
    "                weights = self.creators[1]                  # pesos = weights\n",
    "                # c0 = self.creators[0]                       # activaciones => layer\n",
    "                # c1 = self.creators[1]                       # pesos = weights\n",
    "                # new = self.grad.mm(c1.transpose())  # grad = delta => delta x weights.T\n",
    "                new = Tensor.mm(self.grad, weights.transpose())  # grad = delta => delta x weights.T\n",
    "                layer.backward(new)\n",
    "                # c0.backward(new)                            \n",
    "                # new = self.grad.transpose().mm(c0).transpose() # (delta.T x layer).T = layer.T x delta\n",
    "                new = Tensor.mm(layer.transpose(), self.grad)  # layer.T x delta\n",
    "                weights.backward(new)\n",
    "                # c1.backward(new)\n",
    "\n",
    "            if(self.creation_op == \"transpose\"):\n",
    "                self.creators[0].backward(self.grad.transpose())\n",
    "\n",
    "            if(\"sum\" in self.creation_op):\n",
    "                dim = int(self.creation_op.split(\"_\")[1])\n",
    "                self.creators[0].backward(self.grad.expand(dim, self.creators[0].data.shape[dim]))\n",
    "\n",
    "            if(\"expand\" in self.creation_op):\n",
    "                dim = int(self.creation_op.split(\"_\")[1])\n",
    "                self.creators[0].backward(self.grad.sum(dim))\n",
    "                \n",
    "                \n",
    "    def __neg__(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data * -1,\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op='neg')\n",
    "        return Tensor(self.data * -1)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        '''\n",
    "        @other: un Tensor\n",
    "        '''\n",
    "        if (self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self, other],\n",
    "                          creation_op='add')\n",
    "        return Tensor(self.data + other.data)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        '''\n",
    "        @other: un Tensor\n",
    "        '''\n",
    "        if (self.autograd and other.autograd):\n",
    "            return Tensor(self.data - other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self, other],\n",
    "                          creation_op='sub')\n",
    "        return Tensor(self.data - other.data)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        '''\n",
    "        @other: un Tensor\n",
    "        '''\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data * other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"mul\")\n",
    "        return Tensor(self.data * other.data)\n",
    "    \n",
    "    def sum(self, dim):\n",
    "        '''\n",
    "        Suma atravez de dimensiones, si tenemos una matriz 2x3 y \n",
    "        aplicamos sum(0) sumara todos los valores de las filas \n",
    "        dando como resultado un vector 1x3, en cambio si se aplica\n",
    "        sum(1) el resultado es un vector 2x1\n",
    "        \n",
    "        @dim: dimension para la suma\n",
    "        '''\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.sum(dim),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sum_\"+str(dim))\n",
    "        return Tensor(self.data.sum(dim))\n",
    "    \n",
    "    def expand(self, dim, copies):\n",
    "        '''\n",
    "        Se utiliza para retropropagar a traves de una suma sum().\n",
    "        Copia datos a lo largo de una dimension\n",
    "        '''\n",
    "\n",
    "        trans_cmd = list(range(0,len(self.data.shape)))\n",
    "        trans_cmd.insert(dim,len(self.data.shape))\n",
    "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)\n",
    "        \n",
    "        if(self.autograd):\n",
    "            return Tensor(new_data,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"expand_\"+str(dim))\n",
    "        return Tensor(new_data)\n",
    "    \n",
    "    def transpose(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.transpose(),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"transpose\")\n",
    "        \n",
    "        return Tensor(self.data.transpose())\n",
    "    \n",
    "    def mm(self, x):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.dot(x.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self,x],\n",
    "                          creation_op=\"mm\")\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "    \n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Como funciona ```sum```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "x = Tensor([[1,2,3],[4,5,6]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 7, 9])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 15])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Como funciona ```expand```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 2, 3],\n",
       "        [4, 5, 6]],\n",
       "\n",
       "       [[1, 2, 3],\n",
       "        [4, 5, 6]],\n",
       "\n",
       "       [[1, 2, 3],\n",
       "        [4, 5, 6]],\n",
       "\n",
       "       [[1, 2, 3],\n",
       "        [4, 5, 6]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.expand(0, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 2, 3],\n",
       "        [1, 2, 3],\n",
       "        [1, 2, 3],\n",
       "        [1, 2, 3]],\n",
       "\n",
       "       [[4, 5, 6],\n",
       "        [4, 5, 6],\n",
       "        [4, 5, 6],\n",
       "        [4, 5, 6]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.expand(1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 1, 1, 1],\n",
       "        [2, 2, 2, 2],\n",
       "        [3, 3, 3, 3]],\n",
       "\n",
       "       [[4, 4, 4, 4],\n",
       "        [5, 5, 5, 5],\n",
       "        [6, 6, 6, 6]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.expand(2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuición detras de ```mm``` o ```matrix multiplication```\n",
    "Una sola iteracion de ```forward``` y ```backward``` pass para entender detrás de ```mm``` durante la ejecución de ```backward()```\n",
    "### ```forward pass```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Tensor([[1,1]], autograd=True)\n",
    "y = Tensor([1], autograd=True)\n",
    "weight_0_1 = Tensor([[1,1,1,1],[1,1,1,1]], autograd=True)\n",
    "weight_1_2 = Tensor([[1],[1],[1],[1]], autograd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2) (2, 4) (4, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x.data.shape, weight_0_1.data.shape, weight_1_2.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4) (1, 1)\n"
     ]
    }
   ],
   "source": [
    "layer_1 = x.mm(weight_0_1)\n",
    "layer_2 = layer_1.mm(weight_1_2)\n",
    "print(layer_1.data.shape, layer_2.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```backward pass```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8]] [1] [[7]]\n"
     ]
    }
   ],
   "source": [
    "layer_2_delta = layer_2 - y\n",
    "print(layer_2, y, layer_2_delta)\n",
    "grad = layer_2_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculo de delta o gradiente de la capa actual tomando el ```grad``` de la capa anterior:\n",
    "\n",
    "```grad * weight.T```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 7 7 7]]\n"
     ]
    }
   ],
   "source": [
    "grad_1 = grad.mm(weight_1_2.transpose())\n",
    "print(grad_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 7 7 7]]\n"
     ]
    }
   ],
   "source": [
    "grad_1 = Tensor.mm(grad, weight_1_2.transpose())\n",
    "print(grad_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dos maneras de calcular la actualizacion de los pesos de una red neuronal, es decir ```layer.T x grad```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14]\n",
      " [14]\n",
      " [14]\n",
      " [14]] (4, 1) True\n"
     ]
    }
   ],
   "source": [
    "weight_1_2_update = grad.transpose().mm(layer_1).transpose()\n",
    "print(weight_1_2_update, weight_1_2_update.data.shape ,weight_1_2_update.autograd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14]\n",
      " [14]\n",
      " [14]\n",
      " [14]] (4, 1) True\n"
     ]
    }
   ],
   "source": [
    "weight_1_2_update = Tensor.mm(layer_1.transpose(), grad)\n",
    "print(weight_1_2_update, weight_1_2_update.data.shape ,weight_1_2_update.autograd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
