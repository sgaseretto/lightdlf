{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Añadiendo la posibilidad de usar multiples veces el mismo tensor\n",
    "Como se vio al final del [notebook](02-intro-autograd.ipynb) anterior, si un tensor participa en la creacion de más de un tensor, su gradiente no se acumula, simplemente sobreescribe el gradiente con el ultimo gradiente recibido por el tensor. \n",
    "\n",
    "Para que un tensor pueda participar en la creacion de más de un tensor y mantener correctamente su gradiente es necesario añadir una nueva funcion y actualizar otras tres.\n",
    "\n",
    "Primero que nada los gradientes tienen que poder ser acumulables, permitiendo que si un tensor es usado más de una vez, pueda recibir el gradiente de todos sus hijos (tensores que se originan a partir de el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor(object):\n",
    "    \n",
    "    def __init__(self, data, \n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        '''\n",
    "        Inicializa un tensor utilizando numpy\n",
    "        \n",
    "        @data: una lista de numeros\n",
    "        @creators: lista de tensores que participarion en la creacion de un nuevo tensor\n",
    "        @creators_op: la operacion utilizada para combinar los tensores en el nuevo tensor\n",
    "        '''\n",
    "        self.data = np.array(data)\n",
    "        self.creation_op = creation_op\n",
    "        self.creators = creators\n",
    "        self.grad = None\n",
    "        self.autograd = autograd\n",
    "        self.children = {}\n",
    "        # se asigna un id al tensor\n",
    "        if(id is None):\n",
    "            id = np.random.randint(0,100000)\n",
    "        self.id = id\n",
    "        \n",
    "        # se hace un seguimiento de cuantos hijos tiene un tensor\n",
    "        # si los creadores no es none\n",
    "        if (creators is not None):\n",
    "            # para cada tensor padre\n",
    "            for c in creators:\n",
    "                # se verifica si el tensor padre posee el id del tensor hijo\n",
    "                # en caso de no estar, agrega el id del tensor hijo al tensor padre\n",
    "                if(self.id not in c.children):\n",
    "                    c.children[self.id] = 1\n",
    "                # si el tensor ya se encuentra entre los hijos del padre\n",
    "                # y vuelve a aparece, se incrementa en uno\n",
    "                # la cantidad de apariciones del tensor hijo\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "                    \n",
    "    def all_children_grads_accounted_for(self):\n",
    "        '''\n",
    "        Verifica si un tensor ha recibido la cantidad \n",
    "        correcta de gradientes por cada uno de sus hijos\n",
    "        '''\n",
    "        # print('tensor id:', self.id)\n",
    "        for id, cnt in self.children.items():\n",
    "            print(' all_children_grads_accounted_for id:',id,'cnt:', cnt)\n",
    "            if(cnt != 0):\n",
    "                return False\n",
    "        return True\n",
    "        \n",
    "    def backward(self, grad, grad_origin=None):\n",
    "        '''\n",
    "        Funcion que propaga recursivamente el gradiente a los creators o padres del tensor\n",
    "        \n",
    "        @grad: gradiente \n",
    "        @grad_orign\n",
    "        '''\n",
    "        if(self.autograd):\n",
    "            if(grad_origin is not None):\n",
    "                # Verifica para asegurar si se puede hacer retropropagacion\n",
    "                if(self.children[grad_origin.id] == 0):\n",
    "                    raise Exception(\"No se puede retropropagar mas de una vez\")\n",
    "                # o si se está esperando un gradiente, en dicho caso se decrementa\n",
    "                else:\n",
    "                    # el contador para ese hijo\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "        \n",
    "        # acumula el gradiente de multiples hijos\n",
    "        if(self.grad is None):\n",
    "            self.grad = grad\n",
    "        else:\n",
    "            self.grad += grad\n",
    "        \n",
    "        \n",
    "        print('\\nTensor id',self.id, '\\n',\n",
    "              'Has creators?', self.creators is not None, '\\n',\n",
    "              'All children grads accounted for is', self.all_children_grads_accounted_for(), '\\n',\n",
    "              'grad origin is None?', grad_origin is None\n",
    "             )\n",
    "        if(self.creators is not None and\n",
    "          (self.all_children_grads_accounted_for() or grad_origin is None)):\n",
    "            \n",
    "            if (self.creation_op == 'add'):\n",
    "                # al recibir self.grad, empieza a realizar backprop\n",
    "                print('', self.id, 'creators are:')\n",
    "                print(' creator', self.creators[0].id, ':', self.creators[0], \n",
    "                      'creator', self.creators[1].id, ':',self.creators[1])\n",
    "                self.creators[0].backward(self.grad, grad_origin=self)\n",
    "                self.creators[1].backward(self.grad, grad_origin=self)\n",
    "                \n",
    "        \n",
    "    def __add__(self, other):\n",
    "        '''\n",
    "        @other: un Tensor\n",
    "        '''\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self, other],\n",
    "                          creation_op='add')\n",
    "        return Tensor(self.data + other.data)\n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tensor id 28071 \n",
      " Has creators? True \n",
      " All children grads accounted for is True \n",
      " grad origin is None? True\n",
      " 28071 creators are:\n",
      " creator 72370 : [3 4 5 6 7] creator 71868 : [7 6 5 4 3]\n",
      " all_children_grads_accounted_for id: 28071 cnt: 0\n",
      "\n",
      "Tensor id 72370 \n",
      " Has creators? True \n",
      " All children grads accounted for is True \n",
      " grad origin is None? False\n",
      " all_children_grads_accounted_for id: 28071 cnt: 0\n",
      " 72370 creators are:\n",
      " creator 1 : [1 2 3 4 5] creator 42364 : [2 2 2 2 2]\n",
      " all_children_grads_accounted_for id: 72370 cnt: 0\n",
      "\n",
      "Tensor id 1 \n",
      " Has creators? False \n",
      " All children grads accounted for is True \n",
      " grad origin is None? False\n",
      " all_children_grads_accounted_for id: 72370 cnt: 0\n",
      " all_children_grads_accounted_for id: 71868 cnt: 1\n",
      "\n",
      "Tensor id 42364 \n",
      " Has creators? False \n",
      " All children grads accounted for is False \n",
      " grad origin is None? False\n",
      " all_children_grads_accounted_for id: 28071 cnt: 0\n",
      "\n",
      "Tensor id 71868 \n",
      " Has creators? True \n",
      " All children grads accounted for is True \n",
      " grad origin is None? False\n",
      " all_children_grads_accounted_for id: 28071 cnt: 0\n",
      " 71868 creators are:\n",
      " creator 42364 : [2 2 2 2 2] creator 98614 : [5 4 3 2 1]\n",
      " all_children_grads_accounted_for id: 72370 cnt: 0\n",
      " all_children_grads_accounted_for id: 71868 cnt: 0\n",
      "\n",
      "Tensor id 42364 \n",
      " Has creators? False \n",
      " All children grads accounted for is True \n",
      " grad origin is None? False\n",
      " all_children_grads_accounted_for id: 71868 cnt: 0\n",
      "\n",
      "Tensor id 98614 \n",
      " Has creators? False \n",
      " All children grads accounted for is True \n",
      " grad origin is None? False\n",
      "[ True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor([1,2,3,4,5], autograd=True)\n",
    "b = Tensor([2,2,2,2,2], autograd=True)\n",
    "c = Tensor([5,4,3,2,1], autograd=True)\n",
    "d = a + b\n",
    "e = b + c\n",
    "f = d + e\n",
    "f.backward(Tensor(np.array([1,1,1,1,1])))\n",
    "print(b.grad.data == np.array([2,2,2,2,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x 39389\n",
      "y 55252\n",
      "z 49032\n",
      "\n",
      "Tensor id 49032 \n",
      " Has creators? True \n",
      " All children grads accounted for is True \n",
      " grad origin is None? True\n",
      " 49032 creators are:\n",
      " creator 55252 : [4 4 4 4] creator 55252 : [4 4 4 4]\n",
      " all_children_grads_accounted_for id: 49032 cnt: 1\n",
      "\n",
      "Tensor id 55252 \n",
      " Has creators? True \n",
      " All children grads accounted for is False \n",
      " grad origin is None? False\n",
      " all_children_grads_accounted_for id: 49032 cnt: 1\n",
      " all_children_grads_accounted_for id: 49032 cnt: 0\n",
      "\n",
      "Tensor id 55252 \n",
      " Has creators? True \n",
      " All children grads accounted for is True \n",
      " grad origin is None? False\n",
      " all_children_grads_accounted_for id: 49032 cnt: 0\n",
      " 55252 creators are:\n",
      " creator 39389 : [2 2 2 2] creator 39389 : [2 2 2 2]\n",
      " all_children_grads_accounted_for id: 55252 cnt: 1\n",
      "\n",
      "Tensor id 39389 \n",
      " Has creators? False \n",
      " All children grads accounted for is False \n",
      " grad origin is None? False\n",
      " all_children_grads_accounted_for id: 55252 cnt: 0\n",
      "\n",
      "Tensor id 39389 \n",
      " Has creators? False \n",
      " All children grads accounted for is True \n",
      " grad origin is None? False\n",
      "\n",
      "x gradient data: [4 4 4 4]\n",
      "\n",
      "Tensor id 49032 \n",
      " Has creators? True \n",
      " All children grads accounted for is True \n",
      " grad origin is None? True\n",
      " 49032 creators are:\n",
      " creator 55252 : [4 4 4 4] creator 55252 : [4 4 4 4]\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "No se puede retropropagar mas de una vez",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-fc84c0cad523>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nx gradient data:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-115-49df05a6fa75>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 print(' creator', self.creators[0].id, ':', self.creators[0], \n\u001b[1;32m     91\u001b[0m                       'creator', self.creators[1].id, ':',self.creators[1])\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_origin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_origin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-115-49df05a6fa75>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;31m# Verifica para asegurar si se puede hacer retropropagacion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrad_origin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No se puede retropropagar mas de una vez\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0;31m# o si se está esperando un gradiente, en dicho caso se decrementa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: No se puede retropropagar mas de una vez"
     ]
    }
   ],
   "source": [
    "x = Tensor([2,2,2,2], autograd=True)\n",
    "y = x + x\n",
    "z = y + y\n",
    "print('x',x.id)\n",
    "print('y', y.id)\n",
    "print('z', z.id)\n",
    "z.backward(Tensor([1,1,1,1]))\n",
    "print('\\nx gradient data:',x.grad.data)\n",
    "z.backward(Tensor([1,1,1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
