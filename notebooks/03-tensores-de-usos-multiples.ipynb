{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Añadiendo la posibilidad de usar multiples veces el mismo tensor\n",
    "Como se vio al final del [notebook](02-intro-autograd.ipynb) anterior, si un tensor participa en la creacion de más de un tensor, su gradiente no se acumula, simplemente sobreescribe el gradiente con el ultimo gradiente recibido por el tensor. \n",
    "\n",
    "Para que un tensor pueda participar en la creacion de más de un tensor y mantener correctamente su gradiente es necesario añadir una nueva funcion y actualizar otras tres.\n",
    "\n",
    "Primero que nada los gradientes tienen que poder ser acumulables, permitiendo que si un tensor es usado más de una vez, pueda recibir el gradiente de todos sus hijos (tensores que se originan a partir de el).\n",
    "Adicionalmente se debe crear un contador que permite saber el número de gradientes recibidos por cada uno de los ```hijos``` o tensores creados a partir de los iniciales. Con este conteo también se previene retropropagar el gradiente del mismo hijo dos veces.\n",
    "\n",
    "También el método ```all_children_accounted_for()``` se utiiza para computar si un tensor recibió el gradiente de todos sus hijos en el grafo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor(object):\n",
    "    \n",
    "    def __init__(self, data, \n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        '''\n",
    "        Inicializa un tensor utilizando numpy\n",
    "        \n",
    "        @data: una lista de numeros\n",
    "        @creators: lista de tensores que participarion en la creacion de un nuevo tensor\n",
    "        @creators_op: la operacion utilizada para combinar los tensores en el nuevo tensor\n",
    "        '''\n",
    "        self.data = np.array(data)\n",
    "        self.creation_op = creation_op\n",
    "        self.creators = creators\n",
    "        self.grad = None\n",
    "        self.autograd = autograd\n",
    "        self.children = {}\n",
    "        # se asigna un id al tensor\n",
    "        if(id is None):\n",
    "            id = np.random.randint(0,100000)\n",
    "        self.id = id\n",
    "        \n",
    "        # se hace un seguimiento de cuantos hijos tiene un tensor\n",
    "        # si los creadores no es none\n",
    "        if (creators is not None):\n",
    "            # para cada tensor padre\n",
    "            for c in creators:\n",
    "                # se verifica si el tensor padre posee el id del tensor hijo\n",
    "                # en caso de no estar, agrega el id del tensor hijo al tensor padre\n",
    "                if(self.id not in c.children):\n",
    "                    c.children[self.id] = 1\n",
    "                # si el tensor ya se encuentra entre los hijos del padre\n",
    "                # y vuelve a aparece, se incrementa en uno\n",
    "                # la cantidad de apariciones del tensor hijo\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "                    \n",
    "    def all_children_grads_accounted_for(self, tab='', print_call=True):\n",
    "        '''\n",
    "        Verifica si un tensor ha recibido la cantidad \n",
    "        correcta de gradientes por cada uno de sus hijos\n",
    "        '''\n",
    "        # print('tensor id:', self.id)\n",
    "        for id, cnt in self.children.items():\n",
    "            if (print_call) :\n",
    "                print(tab+'Tensor actual:', self.id, 'hijo:', id, 'count', cnt)\n",
    "            if(cnt != 0):\n",
    "                return False\n",
    "        return True\n",
    "        \n",
    "    def backward(self, grad, grad_origin=None, tab=''):\n",
    "        '''\n",
    "        Funcion que propaga recursivamente el gradiente a los creators o padres del tensor\n",
    "        \n",
    "        @grad: gradiente \n",
    "        @grad_orign\n",
    "        '''\n",
    "#         tab=tab\n",
    "        if(self.autograd):\n",
    "            if(grad_origin is not None):\n",
    "                print(tab+'El gradiente de',self.id,'proviene de (grad_origin):',grad_origin.id, 'count:', self.children[grad_origin.id])\n",
    "                # Verifica para asegurar si se puede hacer retropropagacion\n",
    "                if(self.children[grad_origin.id] == 0):\n",
    "                    raise Exception(\"No se puede retropropagar mas de una vez\")\n",
    "                # o si se está esperando un gradiente, en dicho caso se decrementa\n",
    "                else:\n",
    "                    # el contador para ese hijo\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "                    print(tab+'por tanto el contador de',self.id,'se reduce a', self.children[grad_origin.id], 'para su hijo', grad_origin.id)\n",
    "        \n",
    "        # acumula el gradiente de multiples hijos\n",
    "        if(self.grad is None):\n",
    "            self.grad = grad\n",
    "        else:\n",
    "            self.grad += grad\n",
    "        \n",
    "        \n",
    "        print(tab+'Tensor', self.id, 'has creators?', self.creators is not None,\n",
    "              '\\n'+tab+'All children grads from', self.id,'accounted for is (cnt != 0)', self.all_children_grads_accounted_for(tab=tab, print_call=False),\n",
    "              '\\n'+tab+'Has grad origin?', grad_origin is None,\n",
    "              '\\n'+tab+'Has creators and (children grads accounted or grad no grad origin)',\n",
    "              '\\n'+tab, self.creators is not None, 'and', '(',self.all_children_grads_accounted_for(print_call=False) ,'or',grad_origin is None,') =>',\n",
    "              self.creators is not None and (self.all_children_grads_accounted_for(print_call=False) or grad_origin is None)\n",
    "             )\n",
    "        if(self.creators is not None and\n",
    "          (self.all_children_grads_accounted_for(print_call=False) or grad_origin is None)):\n",
    "            \n",
    "            if (self.creation_op == 'add'):\n",
    "                # al recibir self.grad, empieza a realizar backprop\n",
    "                print(tab + str(self.id), 'creators are:')\n",
    "                print(tab+'creator', self.creators[0].id, ':', self.creators[0], \n",
    "                      'creator', self.creators[1].id, ':',self.creators[1])\n",
    "                print(tab+'\\tbackward call from creator[0]:', self.creators[0].id)\n",
    "                self.creators[0].backward(self.grad, grad_origin=self, tab=tab+'\\t')\n",
    "                print()\n",
    "                print(tab+'\\tbackward call from creator[1]', self.creators[0].id)\n",
    "                self.creators[1].backward(self.grad, grad_origin=self, tab=tab+'\\t')\n",
    "                \n",
    "        \n",
    "    def __add__(self, other):\n",
    "        '''\n",
    "        @other: un Tensor\n",
    "        '''\n",
    "        if(self.autograd and other.autograd):\n",
    "            new_tensor = Tensor(self.data + other.data, \n",
    "                                autograd=True,\n",
    "                                creators=[self, other],\n",
    "                                creation_op='add')\n",
    "            print('  new tensor id is', new_tensor.id)\n",
    "            return new_tensor\n",
    "        return Tensor(self.data + other.data)\n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizacion de las llamadas de backward\n",
    "Se agrego al metodo backward una serie de ```print()```s que permiten ver las llamadas a medida que se propaga el gradiente por cada uno de los tensores que participaron en la creación del tensor final, es decir la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = x + x\n",
      "  new tensor id is 64107\n",
      "z = y + y\n",
      "  new tensor id is 30109\n",
      "\n",
      "x id: 46129\n",
      " hijo: 64107 count 2\n",
      "y id: 64107\n",
      " hijo: 30109 count 2\n",
      "z id: 30109 \n",
      "\n",
      "Tensor 30109 has creators? True \n",
      "All children grads from 30109 accounted for is (cnt != 0) True \n",
      "Has grad origin? True \n",
      "Has creators and (children grads accounted or grad no grad origin) \n",
      " True and ( True or True ) => True\n",
      "30109 creators are:\n",
      "creator 64107 : [4 4 4 4] creator 64107 : [4 4 4 4]\n",
      "\tbackward call from creator[0]: 64107\n",
      "\tEl gradiente de 64107 proviene de (grad_origin): 30109 count: 2\n",
      "\tpor tanto el contador de 64107 se reduce a 1 para su hijo 30109\n",
      "\tTensor 64107 has creators? True \n",
      "\tAll children grads from 64107 accounted for is (cnt != 0) False \n",
      "\tHas grad origin? False \n",
      "\tHas creators and (children grads accounted or grad no grad origin) \n",
      "\t True and ( False or False ) => False\n",
      "\n",
      "\tbackward call from creator[1] 64107\n",
      "\tEl gradiente de 64107 proviene de (grad_origin): 30109 count: 1\n",
      "\tpor tanto el contador de 64107 se reduce a 0 para su hijo 30109\n",
      "\tTensor 64107 has creators? True \n",
      "\tAll children grads from 64107 accounted for is (cnt != 0) True \n",
      "\tHas grad origin? False \n",
      "\tHas creators and (children grads accounted or grad no grad origin) \n",
      "\t True and ( True or False ) => True\n",
      "\t64107 creators are:\n",
      "\tcreator 46129 : [2 2 2 2] creator 46129 : [2 2 2 2]\n",
      "\t\tbackward call from creator[0]: 46129\n",
      "\t\tEl gradiente de 46129 proviene de (grad_origin): 64107 count: 2\n",
      "\t\tpor tanto el contador de 46129 se reduce a 1 para su hijo 64107\n",
      "\t\tTensor 46129 has creators? False \n",
      "\t\tAll children grads from 46129 accounted for is (cnt != 0) False \n",
      "\t\tHas grad origin? False \n",
      "\t\tHas creators and (children grads accounted or grad no grad origin) \n",
      "\t\t False and ( False or False ) => False\n",
      "\n",
      "\t\tbackward call from creator[1] 46129\n",
      "\t\tEl gradiente de 46129 proviene de (grad_origin): 64107 count: 1\n",
      "\t\tpor tanto el contador de 46129 se reduce a 0 para su hijo 64107\n",
      "\t\tTensor 46129 has creators? False \n",
      "\t\tAll children grads from 46129 accounted for is (cnt != 0) True \n",
      "\t\tHas grad origin? False \n",
      "\t\tHas creators and (children grads accounted or grad no grad origin) \n",
      "\t\t False and ( True or False ) => False\n",
      "\n",
      "x gradient data: [4 4 4 4]\n"
     ]
    }
   ],
   "source": [
    "x = Tensor([2,2,2,2], autograd=True)\n",
    "print('y = x + x')\n",
    "y = x + x\n",
    "print('z = y + y')\n",
    "z = y + y\n",
    "print()\n",
    "\n",
    "print('x id:',x.id)\n",
    "for hijo, cnt in x.children.items():\n",
    "    print(' hijo:', hijo, 'count', cnt)\n",
    "    \n",
    "print('y id:', y.id)\n",
    "for hijo, cnt in y.children.items():\n",
    "    print(' hijo:', hijo, 'count', cnt)\n",
    "    \n",
    "print('z id:', z.id, '\\n')\n",
    "z.backward(Tensor([1,1,1,1]))\n",
    "print('\\nx gradient data:',x.grad.data)\n",
    "# z.backward(Tensor([1,1,1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d = a + b\n",
      "  new tensor id is 88946\n",
      "e = b + c\n",
      "  new tensor id is 24791\n",
      "f = d + e\n",
      "  new tensor id is 70343\n",
      "\n",
      "a id: 63152\n",
      "\thijo: 88946 count 1\n",
      "b id: 77066\n",
      "\thijo: 88946 count 1\n",
      "\thijo: 24791 count 1\n",
      "c id: 13522\n",
      "\thijo: 24791 count 1\n",
      "d id: 88946\n",
      "\thijo: 70343 count 1\n",
      "e id: 24791\n",
      "\thijo: 70343 count 1\n",
      "f id: 70343 \n",
      "\n",
      "Tensor 70343 has creators? True \n",
      "All children grads from 70343 accounted for is (cnt != 0) True \n",
      "Has grad origin? True \n",
      "Has creators and (children grads accounted or grad no grad origin) \n",
      " True and ( True or True ) => True\n",
      "70343 creators are:\n",
      "creator 88946 : [3 4 5 6 7] creator 24791 : [7 6 5 4 3]\n",
      "\tbackward call from creator[0]: 88946\n",
      "\tEl gradiente de 88946 proviene de (grad_origin): 70343 count: 1\n",
      "\tpor tanto el contador de 88946 se reduce a 0 para su hijo 70343\n",
      "\tTensor 88946 has creators? True \n",
      "\tAll children grads from 88946 accounted for is (cnt != 0) True \n",
      "\tHas grad origin? False \n",
      "\tHas creators and (children grads accounted or grad no grad origin) \n",
      "\t True and ( True or False ) => True\n",
      "\t88946 creators are:\n",
      "\tcreator 63152 : [1 2 3 4 5] creator 77066 : [2 2 2 2 2]\n",
      "\t\tbackward call from creator[0]: 63152\n",
      "\t\tEl gradiente de 63152 proviene de (grad_origin): 88946 count: 1\n",
      "\t\tpor tanto el contador de 63152 se reduce a 0 para su hijo 88946\n",
      "\t\tTensor 63152 has creators? False \n",
      "\t\tAll children grads from 63152 accounted for is (cnt != 0) True \n",
      "\t\tHas grad origin? False \n",
      "\t\tHas creators and (children grads accounted or grad no grad origin) \n",
      "\t\t False and ( True or False ) => False\n",
      "\n",
      "\t\tbackward call from creator[1] 63152\n",
      "\t\tEl gradiente de 77066 proviene de (grad_origin): 88946 count: 1\n",
      "\t\tpor tanto el contador de 77066 se reduce a 0 para su hijo 88946\n",
      "\t\tTensor 77066 has creators? False \n",
      "\t\tAll children grads from 77066 accounted for is (cnt != 0) False \n",
      "\t\tHas grad origin? False \n",
      "\t\tHas creators and (children grads accounted or grad no grad origin) \n",
      "\t\t False and ( False or False ) => False\n",
      "\n",
      "\tbackward call from creator[1] 88946\n",
      "\tEl gradiente de 24791 proviene de (grad_origin): 70343 count: 1\n",
      "\tpor tanto el contador de 24791 se reduce a 0 para su hijo 70343\n",
      "\tTensor 24791 has creators? True \n",
      "\tAll children grads from 24791 accounted for is (cnt != 0) True \n",
      "\tHas grad origin? False \n",
      "\tHas creators and (children grads accounted or grad no grad origin) \n",
      "\t True and ( True or False ) => True\n",
      "\t24791 creators are:\n",
      "\tcreator 77066 : [2 2 2 2 2] creator 13522 : [5 4 3 2 1]\n",
      "\t\tbackward call from creator[0]: 77066\n",
      "\t\tEl gradiente de 77066 proviene de (grad_origin): 24791 count: 1\n",
      "\t\tpor tanto el contador de 77066 se reduce a 0 para su hijo 24791\n",
      "\t\tTensor 77066 has creators? False \n",
      "\t\tAll children grads from 77066 accounted for is (cnt != 0) True \n",
      "\t\tHas grad origin? False \n",
      "\t\tHas creators and (children grads accounted or grad no grad origin) \n",
      "\t\t False and ( True or False ) => False\n",
      "\n",
      "\t\tbackward call from creator[1] 77066\n",
      "\t\tEl gradiente de 13522 proviene de (grad_origin): 24791 count: 1\n",
      "\t\tpor tanto el contador de 13522 se reduce a 0 para su hijo 24791\n",
      "\t\tTensor 13522 has creators? False \n",
      "\t\tAll children grads from 13522 accounted for is (cnt != 0) True \n",
      "\t\tHas grad origin? False \n",
      "\t\tHas creators and (children grads accounted or grad no grad origin) \n",
      "\t\t False and ( True or False ) => False\n",
      "[ True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor([1,2,3,4,5], autograd=True)\n",
    "b = Tensor([2,2,2,2,2], autograd=True)\n",
    "c = Tensor([5,4,3,2,1], autograd=True)\n",
    "\n",
    "print('d = a + b')\n",
    "d = a + b\n",
    "print('e = b + c')\n",
    "e = b + c\n",
    "print('f = d + e')\n",
    "f = d + e\n",
    "print()\n",
    "\n",
    "print('a id:',a.id)\n",
    "for hijo, cnt in a.children.items():\n",
    "    print('\\thijo:', hijo, 'count', cnt)\n",
    "    \n",
    "print('b id:', b.id)\n",
    "for hijo, cnt in b.children.items():\n",
    "    print('\\thijo:', hijo, 'count', cnt)\n",
    "    \n",
    "print('c id:',c.id)\n",
    "for hijo, cnt in c.children.items():\n",
    "    print('\\thijo:', hijo, 'count', cnt)\n",
    "    \n",
    "print('d id:', d.id)\n",
    "for hijo, cnt in d.children.items():\n",
    "    print('\\thijo:', hijo, 'count', cnt)\n",
    "\n",
    "print('e id:', e.id)\n",
    "for hijo, cnt in e.children.items():\n",
    "    print('\\thijo:', hijo, 'count', cnt)\n",
    "    \n",
    "print('f id:', f.id, '\\n')\n",
    "\n",
    "f.backward(Tensor(np.array([1,1,1,1,1])))\n",
    "print(b.grad.data == np.array([2,2,2,2,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x3 = x1 + x2\n",
      "  new tensor id is 30107\n",
      "x4 = x1 + x2\n",
      "  new tensor id is 57918\n",
      "x5 = x1+ x2 + x3 + x4\n",
      "  new tensor id is 23724\n",
      "  new tensor id is 36893\n",
      "  new tensor id is 92965\n",
      "\n",
      "x1 id: 62923\n",
      "\thijo: 30107 count 1\n",
      "\thijo: 57918 count 1\n",
      "\thijo: 23724 count 1\n",
      "x2 id: 36195\n",
      "\thijo: 30107 count 1\n",
      "\thijo: 57918 count 1\n",
      "\thijo: 23724 count 1\n",
      "x3 id: 30107\n",
      "\thijo: 36893 count 1\n",
      "x4 id: 57918\n",
      "\thijo: 92965 count 1\n",
      "x5 id: 92965 \n",
      "\n",
      "Tensor 92965 has creators? True \n",
      "All children grads from 92965 accounted for is (cnt != 0) True \n",
      "Has grad origin? True \n",
      "Has creators and (children grads accounted or grad no grad origin) \n",
      " True and ( True or True ) => True\n",
      "92965 creators are:\n",
      "creator 36893 : [4 4 4 4] creator 57918 : [2 2 2 2]\n",
      "\tbackward call from creator[0]: 36893\n",
      "\tEl gradiente de 36893 proviene de (grad_origin): 92965 count: 1\n",
      "\tpor tanto el contador de 36893 se reduce a 0 para su hijo 92965\n",
      "\tTensor 36893 has creators? True \n",
      "\tAll children grads from 36893 accounted for is (cnt != 0) True \n",
      "\tHas grad origin? False \n",
      "\tHas creators and (children grads accounted or grad no grad origin) \n",
      "\t True and ( True or False ) => True\n",
      "\t36893 creators are:\n",
      "\tcreator 23724 : [2 2 2 2] creator 30107 : [2 2 2 2]\n",
      "\t\tbackward call from creator[0]: 23724\n",
      "\t\tEl gradiente de 23724 proviene de (grad_origin): 36893 count: 1\n",
      "\t\tpor tanto el contador de 23724 se reduce a 0 para su hijo 36893\n",
      "\t\tTensor 23724 has creators? True \n",
      "\t\tAll children grads from 23724 accounted for is (cnt != 0) True \n",
      "\t\tHas grad origin? False \n",
      "\t\tHas creators and (children grads accounted or grad no grad origin) \n",
      "\t\t True and ( True or False ) => True\n",
      "\t\t23724 creators are:\n",
      "\t\tcreator 62923 : [1 1 1 1] creator 36195 : [1 1 1 1]\n",
      "\t\t\tbackward call from creator[0]: 62923\n",
      "\t\t\tEl gradiente de 62923 proviene de (grad_origin): 23724 count: 1\n",
      "\t\t\tpor tanto el contador de 62923 se reduce a 0 para su hijo 23724\n",
      "\t\t\tTensor 62923 has creators? False \n",
      "\t\t\tAll children grads from 62923 accounted for is (cnt != 0) False \n",
      "\t\t\tHas grad origin? False \n",
      "\t\t\tHas creators and (children grads accounted or grad no grad origin) \n",
      "\t\t\t False and ( False or False ) => False\n",
      "\n",
      "\t\t\tbackward call from creator[1] 62923\n",
      "\t\t\tEl gradiente de 36195 proviene de (grad_origin): 23724 count: 1\n",
      "\t\t\tpor tanto el contador de 36195 se reduce a 0 para su hijo 23724\n",
      "\t\t\tTensor 36195 has creators? False \n",
      "\t\t\tAll children grads from 36195 accounted for is (cnt != 0) False \n",
      "\t\t\tHas grad origin? False \n",
      "\t\t\tHas creators and (children grads accounted or grad no grad origin) \n",
      "\t\t\t False and ( False or False ) => False\n",
      "\n",
      "\t\tbackward call from creator[1] 23724\n",
      "\t\tEl gradiente de 30107 proviene de (grad_origin): 36893 count: 1\n",
      "\t\tpor tanto el contador de 30107 se reduce a 0 para su hijo 36893\n",
      "\t\tTensor 30107 has creators? True \n",
      "\t\tAll children grads from 30107 accounted for is (cnt != 0) True \n",
      "\t\tHas grad origin? False \n",
      "\t\tHas creators and (children grads accounted or grad no grad origin) \n",
      "\t\t True and ( True or False ) => True\n",
      "\t\t30107 creators are:\n",
      "\t\tcreator 62923 : [1 1 1 1] creator 36195 : [1 1 1 1]\n",
      "\t\t\tbackward call from creator[0]: 62923\n",
      "\t\t\tEl gradiente de 62923 proviene de (grad_origin): 30107 count: 1\n",
      "\t\t\tpor tanto el contador de 62923 se reduce a 0 para su hijo 30107\n",
      "\t\t\tTensor 62923 has creators? False \n",
      "\t\t\tAll children grads from 62923 accounted for is (cnt != 0) False \n",
      "\t\t\tHas grad origin? False \n",
      "\t\t\tHas creators and (children grads accounted or grad no grad origin) \n",
      "\t\t\t False and ( False or False ) => False\n",
      "\n",
      "\t\t\tbackward call from creator[1] 62923\n",
      "\t\t\tEl gradiente de 36195 proviene de (grad_origin): 30107 count: 1\n",
      "\t\t\tpor tanto el contador de 36195 se reduce a 0 para su hijo 30107\n",
      "\t\t\tTensor 36195 has creators? False \n",
      "\t\t\tAll children grads from 36195 accounted for is (cnt != 0) False \n",
      "\t\t\tHas grad origin? False \n",
      "\t\t\tHas creators and (children grads accounted or grad no grad origin) \n",
      "\t\t\t False and ( False or False ) => False\n",
      "\n",
      "\tbackward call from creator[1] 36893\n",
      "\tEl gradiente de 57918 proviene de (grad_origin): 92965 count: 1\n",
      "\tpor tanto el contador de 57918 se reduce a 0 para su hijo 92965\n",
      "\tTensor 57918 has creators? True \n",
      "\tAll children grads from 57918 accounted for is (cnt != 0) True \n",
      "\tHas grad origin? False \n",
      "\tHas creators and (children grads accounted or grad no grad origin) \n",
      "\t True and ( True or False ) => True\n",
      "\t57918 creators are:\n",
      "\tcreator 62923 : [1 1 1 1] creator 36195 : [1 1 1 1]\n",
      "\t\tbackward call from creator[0]: 62923\n",
      "\t\tEl gradiente de 62923 proviene de (grad_origin): 57918 count: 1\n",
      "\t\tpor tanto el contador de 62923 se reduce a 0 para su hijo 57918\n",
      "\t\tTensor 62923 has creators? False \n",
      "\t\tAll children grads from 62923 accounted for is (cnt != 0) True \n",
      "\t\tHas grad origin? False \n",
      "\t\tHas creators and (children grads accounted or grad no grad origin) \n",
      "\t\t False and ( True or False ) => False\n",
      "\n",
      "\t\tbackward call from creator[1] 62923\n",
      "\t\tEl gradiente de 36195 proviene de (grad_origin): 57918 count: 1\n",
      "\t\tpor tanto el contador de 36195 se reduce a 0 para su hijo 57918\n",
      "\t\tTensor 36195 has creators? False \n",
      "\t\tAll children grads from 36195 accounted for is (cnt != 0) True \n",
      "\t\tHas grad origin? False \n",
      "\t\tHas creators and (children grads accounted or grad no grad origin) \n",
      "\t\t False and ( True or False ) => False\n"
     ]
    }
   ],
   "source": [
    "x1 = Tensor([1,1,1,1], autograd=True)\n",
    "x2 = Tensor([1,1,1,1], autograd=True)\n",
    "\n",
    "print('x3 = x1 + x2')\n",
    "x3 = x1 + x2\n",
    "print('x4 = x1 + x2')\n",
    "x4 = x1 + x2\n",
    "print('x5 = x1+ x2 + x3 + x4')\n",
    "x5 = x1+ x2 + x3 + x4\n",
    "print()\n",
    "\n",
    "print('x1 id:',x1.id)\n",
    "for hijo, cnt in x1.children.items():\n",
    "    print('\\thijo:', hijo, 'count', cnt)\n",
    "    \n",
    "print('x2 id:', x2.id)\n",
    "for hijo, cnt in x2.children.items():\n",
    "    print('\\thijo:', hijo, 'count', cnt)\n",
    "    \n",
    "print('x3 id:',x3.id)\n",
    "for hijo, cnt in x3.children.items():\n",
    "    print('\\thijo:', hijo, 'count', cnt)\n",
    "    \n",
    "print('x4 id:', x4.id)\n",
    "for hijo, cnt in x4.children.items():\n",
    "    print('\\thijo:', hijo, 'count', cnt)\n",
    "    \n",
    "print('x5 id:', x5.id, '\\n')\n",
    "\n",
    "x5.backward(Tensor([1,1,1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = x + x + x + x\n",
      "  new tensor id is 40681\n",
      "  new tensor id is 10062\n",
      "  new tensor id is 60762\n",
      "z = y + y\n",
      "  new tensor id is 51449\n",
      "\n",
      "x id: 21051\n",
      " hijo: 40681 count 2\n",
      " hijo: 10062 count 1\n",
      " hijo: 60762 count 1\n",
      "y id: 60762\n",
      " hijo: 51449 count 2\n",
      "z id: 51449 \n",
      "\n",
      "Tensor 51449 has creators? True \n",
      "All children grads from 51449 accounted for is (cnt != 0) True \n",
      "Has grad origin? True \n",
      "Has creators and (children grads accounted or grad no grad origin) \n",
      " True and ( True or True ) => True\n",
      "51449 creators are:\n",
      "creator 60762 : [8 8 8 8] creator 60762 : [8 8 8 8]\n",
      "\tbackward call from creator[0]: 60762\n",
      "\tEl gradiente de 60762 proviene de (grad_origin): 51449 count: 2\n",
      "\tpor tanto el contador de 60762 se reduce a 1 para su hijo 51449\n",
      "\tTensor 60762 has creators? True \n",
      "\tAll children grads from 60762 accounted for is (cnt != 0) False \n",
      "\tHas grad origin? False \n",
      "\tHas creators and (children grads accounted or grad no grad origin) \n",
      "\t True and ( False or False ) => False\n",
      "\n",
      "\tbackward call from creator[1] 60762\n",
      "\tEl gradiente de 60762 proviene de (grad_origin): 51449 count: 1\n",
      "\tpor tanto el contador de 60762 se reduce a 0 para su hijo 51449\n",
      "\tTensor 60762 has creators? True \n",
      "\tAll children grads from 60762 accounted for is (cnt != 0) True \n",
      "\tHas grad origin? False \n",
      "\tHas creators and (children grads accounted or grad no grad origin) \n",
      "\t True and ( True or False ) => True\n",
      "\t60762 creators are:\n",
      "\tcreator 10062 : [6 6 6 6] creator 21051 : [2 2 2 2]\n",
      "\t\tbackward call from creator[0]: 10062\n",
      "\t\tEl gradiente de 10062 proviene de (grad_origin): 60762 count: 1\n",
      "\t\tpor tanto el contador de 10062 se reduce a 0 para su hijo 60762\n",
      "\t\tTensor 10062 has creators? True \n",
      "\t\tAll children grads from 10062 accounted for is (cnt != 0) True \n",
      "\t\tHas grad origin? False \n",
      "\t\tHas creators and (children grads accounted or grad no grad origin) \n",
      "\t\t True and ( True or False ) => True\n",
      "\t\t10062 creators are:\n",
      "\t\tcreator 40681 : [4 4 4 4] creator 21051 : [2 2 2 2]\n",
      "\t\t\tbackward call from creator[0]: 40681\n",
      "\t\t\tEl gradiente de 40681 proviene de (grad_origin): 10062 count: 1\n",
      "\t\t\tpor tanto el contador de 40681 se reduce a 0 para su hijo 10062\n",
      "\t\t\tTensor 40681 has creators? True \n",
      "\t\t\tAll children grads from 40681 accounted for is (cnt != 0) True \n",
      "\t\t\tHas grad origin? False \n",
      "\t\t\tHas creators and (children grads accounted or grad no grad origin) \n",
      "\t\t\t True and ( True or False ) => True\n",
      "\t\t\t40681 creators are:\n",
      "\t\t\tcreator 21051 : [2 2 2 2] creator 21051 : [2 2 2 2]\n",
      "\t\t\t\tbackward call from creator[0]: 21051\n",
      "\t\t\t\tEl gradiente de 21051 proviene de (grad_origin): 40681 count: 2\n",
      "\t\t\t\tpor tanto el contador de 21051 se reduce a 1 para su hijo 40681\n",
      "\t\t\t\tTensor 21051 has creators? False \n",
      "\t\t\t\tAll children grads from 21051 accounted for is (cnt != 0) False \n",
      "\t\t\t\tHas grad origin? False \n",
      "\t\t\t\tHas creators and (children grads accounted or grad no grad origin) \n",
      "\t\t\t\t False and ( False or False ) => False\n",
      "\n",
      "\t\t\t\tbackward call from creator[1] 21051\n",
      "\t\t\t\tEl gradiente de 21051 proviene de (grad_origin): 40681 count: 1\n",
      "\t\t\t\tpor tanto el contador de 21051 se reduce a 0 para su hijo 40681\n",
      "\t\t\t\tTensor 21051 has creators? False \n",
      "\t\t\t\tAll children grads from 21051 accounted for is (cnt != 0) False \n",
      "\t\t\t\tHas grad origin? False \n",
      "\t\t\t\tHas creators and (children grads accounted or grad no grad origin) \n",
      "\t\t\t\t False and ( False or False ) => False\n",
      "\n",
      "\t\t\tbackward call from creator[1] 40681\n",
      "\t\t\tEl gradiente de 21051 proviene de (grad_origin): 10062 count: 1\n",
      "\t\t\tpor tanto el contador de 21051 se reduce a 0 para su hijo 10062\n",
      "\t\t\tTensor 21051 has creators? False \n",
      "\t\t\tAll children grads from 21051 accounted for is (cnt != 0) False \n",
      "\t\t\tHas grad origin? False \n",
      "\t\t\tHas creators and (children grads accounted or grad no grad origin) \n",
      "\t\t\t False and ( False or False ) => False\n",
      "\n",
      "\t\tbackward call from creator[1] 10062\n",
      "\t\tEl gradiente de 21051 proviene de (grad_origin): 60762 count: 1\n",
      "\t\tpor tanto el contador de 21051 se reduce a 0 para su hijo 60762\n",
      "\t\tTensor 21051 has creators? False \n",
      "\t\tAll children grads from 21051 accounted for is (cnt != 0) True \n",
      "\t\tHas grad origin? False \n",
      "\t\tHas creators and (children grads accounted or grad no grad origin) \n",
      "\t\t False and ( True or False ) => False\n",
      "\n",
      "x gradient data: [8 8 8 8]\n"
     ]
    }
   ],
   "source": [
    "x = Tensor([2,2,2,2], autograd=True)\n",
    "\n",
    "print('y = x + x + x + x')\n",
    "y = x + x + x + x\n",
    "print('z = y + y')\n",
    "z = y + y\n",
    "print()\n",
    "\n",
    "print('x id:',x.id)\n",
    "for hijo, cnt in x.children.items():\n",
    "    print(' hijo:', hijo, 'count', cnt)\n",
    "    \n",
    "print('y id:', y.id)\n",
    "for hijo, cnt in y.children.items():\n",
    "    print(' hijo:', hijo, 'count', cnt)\n",
    "    \n",
    "print('z id:', z.id, '\\n')\n",
    "\n",
    "z.backward(Tensor([1,1,1,1]))\n",
    "print('\\nx gradient data:',x.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
