{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agregando como capa una red neuronal recurrente o ```RNN```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinando varias capas es posible aprender sobre series de tiempo\n",
    "La \"capa\" recurrente se construye tres capas lineales, y el método ```forward()``` tomará la salida del estado oculto anterior o ```hidden state``` y la entrada siguiente a ser procesada del conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightdlf.cpu.core import Tensor\n",
    "from lightdlf.cpu.layers import Layer, Linear, Embedding, CrossEntropyLoss, Sigmoid, Tanh\n",
    "from lightdlf.cpu.optimizers import SGD\n",
    "\n",
    "class RNNCell(Layer):\n",
    "    def __init__(self, n_inputs, n_hidden, n_output, activation='sigmoid'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "\n",
    "        if(activation == 'sigmoid'):\n",
    "            self.activation = Sigmoid()\n",
    "        elif(activation == 'tanh'):\n",
    "            self.activation = Tanh()\n",
    "        else:\n",
    "            raise Exception(\"Non-linearity not found\")\n",
    "\n",
    "        self.w_ih = Linear(n_inputs=n_inputs, n_outputs=n_hidden)\n",
    "        self.w_hh = Linear(n_inputs=n_hidden, n_outputs=n_hidden)\n",
    "        self.w_ho = Linear(n_inputs=n_hidden, n_outputs=n_output)\n",
    "\n",
    "        self.parameters += self.w_ih.get_parameters()\n",
    "        self.parameters += self.w_hh.get_parameters()\n",
    "        self.parameters += self.w_ho.get_parameters()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        from_prev_hidden = self.w_hh.forward(hidden)\n",
    "        from_actual_input = self.w_ih.forward(input)\n",
    "        combined =  from_actual_input + from_prev_hidden\n",
    "        new_hidden = self.activation.forward(combined)\n",
    "        output = self.w_ho.forward(new_hidden)\n",
    "        return output, new_hidden\n",
    "\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return Tensor(np.zeros((batch_size, self.n_hidden)), autograd=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las ```RRNs``` tienen un vector de estado que pasa de iteracion a iteracion. Este vector de estado es el vector ```hidden```, que es tanto un parametro de entrada y una variable de salida para la funcion ```forward()```\n",
    "\n",
    "Las ```RNNs``` tienen diferentes matrices de pesos:\n",
    "- ```w_ih```: mapea vectores de entrada a vectores ocultos (procesa datos de entrada)\n",
    "- ```w_hh```: mapea de vectores ocultos a otros vectores ocultos (que actualiza cada vector oculto en base al vector oculto anterior)\n",
    "- ```w_ho```: vector opcional que aprende a hacer predicciones en base al vector oculto\n",
    "\n",
    "```w_ih``` y ```w_ho``` son del tamaño del vocabulario, todas las demas dimensiones son configurables en base al parametro ```n_hidden```.\n",
    "\n",
    "Finalmente, el parametro ```activation``` define la funcion no lineal a utilizar como activación en cada paso o ```timestep```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba de la red neuronal recurrente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,random,math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "f = open('datasets/en/qa1_single-supporting-fact_train.txt','r')\n",
    "raw = f.readlines()\n",
    "f.close()\n",
    "\n",
    "tokens = list()\n",
    "for line in raw[0:1000]:\n",
    "    tokens.append(line.lower().replace(\"\\n\",\"\").split(\" \")[1:])\n",
    "\n",
    "new_tokens = list()\n",
    "for line in tokens:\n",
    "    new_tokens.append(['-'] * (6 - len(line)) + line)\n",
    "\n",
    "tokens = new_tokens\n",
    "\n",
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        vocab.add(word)\n",
    "\n",
    "vocab = list(vocab)\n",
    "\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word]=i\n",
    "    \n",
    "def words2indices(sentence):\n",
    "    idx = list()\n",
    "    for word in sentence:\n",
    "        idx.append(word2index[word])\n",
    "    return idx\n",
    "\n",
    "indices = list()\n",
    "for line in tokens:\n",
    "    idx = list()\n",
    "    for w in line:\n",
    "        idx.append(word2index[w])\n",
    "    indices.append(idx)\n",
    "\n",
    "data = np.array(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.46704709467587635 % Correct: 0.01\n",
      "Loss: 0.17105290314398497 % Correct: 0.27\n",
      "Loss: 0.1589218180596923 % Correct: 0.32\n",
      "Loss: 0.14024067499987708 % Correct: 0.36\n",
      "Loss: 0.13620394302913222 % Correct: 0.37\n"
     ]
    }
   ],
   "source": [
    "embed = Embedding(vocab_size=len(vocab), dim=16)\n",
    "model = RNNCell(n_inputs=16, n_hidden=16, n_output=len(vocab))\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "params = model.get_parameters() + embed.get_parameters()\n",
    "optim = SGD(parameters=params, alpha=0.05)\n",
    "\n",
    "for iter in range(1000):\n",
    "    batch_size = 100\n",
    "    total_loss = 0\n",
    "    \n",
    "    hidden = model.init_hidden(batch_size=100)\n",
    "    \n",
    "    for t in range(5):\n",
    "        input = Tensor(data[0:batch_size,t], autograd=True)\n",
    "        rnn_input = embed.forward(input=input)\n",
    "        output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "        \n",
    "    target = Tensor(data[0:batch_size,t+1], autograd=True)\n",
    "    loss = criterion.forward(output, target)\n",
    "    loss.backward(grad=None)\n",
    "    optim.step()\n",
    "    total_loss += loss.data\n",
    "    if(iter % 200 == 0):\n",
    "        p_correct = (target.data == np.argmax(output.data,axis=1)).mean()\n",
    "        print(\"Loss:\",total_loss / (len(data)/batch_size),\"% Correct:\",p_correct)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
