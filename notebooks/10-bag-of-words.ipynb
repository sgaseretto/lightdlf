{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightdlf_old.cpu.core import Tensor\n",
    "from lightdlf_old.cpu.layers import Embedding\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<lightdlf.cpu.layers.Embedding object at 0x11330a3c8>\n"
     ]
    }
   ],
   "source": [
    "x = Embedding(3,5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09231089, -0.02688597,  0.00918141,  0.00381674, -0.03545999],\n",
       "       [ 0.09436207, -0.03763539, -0.09253179, -0.04245997, -0.00750637],\n",
       "       [-0.06457234,  0.00024291,  0.08645136,  0.0597631 , -0.02688918]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices [0,2]\n",
      " [[-0.09231089 -0.02688597  0.00918141  0.00381674 -0.03545999]\n",
      " [-0.06457234  0.00024291  0.08645136  0.0597631  -0.02688918]]\n",
      "indices [1,2]\n",
      " [[ 0.09436207 -0.03763539 -0.09253179 -0.04245997 -0.00750637]\n",
      " [-0.06457234  0.00024291  0.08645136  0.0597631  -0.02688918]]\n"
     ]
    }
   ],
   "source": [
    "print('indices [0,2]\\n',x.weight.index_select(Tensor([[0,2],[1,2]])).data[0])\n",
    "print('indices [1,2]\\n',x.weight.index_select(Tensor([[0,2],[1,2]])).data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.weight.index_select(Tensor([0,2])).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.15688323 -0.02664306  0.09563277  0.06357984 -0.06234917]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.15688323 -0.02664306  0.09563277  0.06357984 -0.06234917]\n",
      " [ 0.02978974 -0.03739248 -0.00608043  0.01730313 -0.03439555]]\n"
     ]
    }
   ],
   "source": [
    "y = x.weight.index_select(Tensor([[0,2],[1,2]])).sum(1)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Palabras utilizadas para un Bag of Words simple\n",
    "Palabras con sus indices:\n",
    "0. buen \n",
    "1. trabajo\n",
    "2. sabrosa \n",
    "3. comida\n",
    "4. fiesta \n",
    "5. divertida\n",
    "6. reunion \n",
    "7. aburrida\n",
    "8. trabajo \n",
    "9. dificil\n",
    "10. grave \n",
    "11. error\n",
    "\n",
    "Oraciones:\n",
    "- buen trabajo\n",
    "- sabrosa comida\n",
    "- fiesta divertida\n",
    "- reunion aburrida\n",
    "- trabajo dificil\n",
    "- grave error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01129587  0.05411751 -0.01789828  0.0478349  -0.0959201 ]\n",
      " [ 0.08772247  0.06548647 -0.08466379  0.06105468 -0.04552785]\n",
      " [ 0.03344448 -0.01799966  0.06945104  0.07558596 -0.07014028]\n",
      " [ 0.02046598 -0.03709017 -0.04655184 -0.06250056 -0.04449235]\n",
      " [-0.08805942  0.07226693 -0.01671041  0.09310067  0.01683078]\n",
      " [ 0.0757382  -0.0545573   0.09674004  0.08712794 -0.05208956]\n",
      " [ 0.00836074 -0.088845   -0.01288659 -0.03770544 -0.02679251]\n",
      " [ 0.05411948  0.09297315  0.08984485  0.07841951  0.03230447]\n",
      " [ 0.05794195 -0.03361713 -0.02556826 -0.02620728 -0.07834703]\n",
      " [ 0.0523658   0.03139289  0.01005846 -0.06450427  0.06012531]\n",
      " [-0.09164736 -0.05735323 -0.05880776  0.08320324  0.05173261]\n",
      " [-0.03472729  0.07937562 -0.09114472  0.03899011  0.09192391]]\n"
     ]
    }
   ],
   "source": [
    "data = Tensor([[0,1],\n",
    "               [2,3],\n",
    "               [4,5],\n",
    "               [6,7],\n",
    "               [8,9],\n",
    "               [10,11]], \n",
    "              autograd=True)\n",
    "\n",
    "target = Tensor([[1],[1],[1],[0],[0],[0]], autograd=True)\n",
    "\n",
    "embed = Embedding(12,5)\n",
    "weight = Tensor(np.random.rand(5,1), autograd=True)\n",
    "print(embed.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set = embed.weight.index_select(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.01129587  0.05411751 -0.01789828  0.0478349  -0.0959201 ]\n",
      "  [ 0.08772247  0.06548647 -0.08466379  0.06105468 -0.04552785]]\n",
      "\n",
      " [[ 0.03344448 -0.01799966  0.06945104  0.07558596 -0.07014028]\n",
      "  [ 0.02046598 -0.03709017 -0.04655184 -0.06250056 -0.04449235]]\n",
      "\n",
      " [[-0.08805942  0.07226693 -0.01671041  0.09310067  0.01683078]\n",
      "  [ 0.0757382  -0.0545573   0.09674004  0.08712794 -0.05208956]]\n",
      "\n",
      " [[ 0.00836074 -0.088845   -0.01288659 -0.03770544 -0.02679251]\n",
      "  [ 0.05411948  0.09297315  0.08984485  0.07841951  0.03230447]]\n",
      "\n",
      " [[ 0.05794195 -0.03361713 -0.02556826 -0.02620728 -0.07834703]\n",
      "  [ 0.0523658   0.03139289  0.01005846 -0.06450427  0.06012531]]\n",
      "\n",
      " [[-0.09164736 -0.05735323 -0.05880776  0.08320324  0.05173261]\n",
      "  [-0.03472729  0.07937562 -0.09114472  0.03899011  0.09192391]]]\n"
     ]
    }
   ],
   "source": [
    "print(word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.09901833  0.11960399 -0.10256207  0.10888958 -0.14144795]\n",
      " [ 0.05391046 -0.05508983  0.02289919  0.0130854  -0.11463264]\n",
      " [-0.01232122  0.01770963  0.08002963  0.18022861 -0.03525878]\n",
      " [ 0.06248022  0.00412815  0.07695827  0.04071407  0.00551196]\n",
      " [ 0.11030776 -0.00222424 -0.0155098  -0.09071155 -0.01822172]\n",
      " [-0.12637465  0.02202239 -0.14995248  0.12219334  0.14365652]]\n"
     ]
    }
   ],
   "source": [
    "bag = word_set.sum(1)\n",
    "print(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.09221484]\n",
      " [ 0.02066811]\n",
      " [ 0.10238517]\n",
      " [ 0.098156  ]\n",
      " [ 0.03204621]\n",
      " [-0.07952762]]\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "pred = Tensor.mm(bag,weight)\n",
    "print(pred)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = ((pred - target) * (pred - target)).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward(grad=None)\n",
    "weight.data = weight.data + 0.005 * weight.grad.data\n",
    "weight.grad.data *= 0\n",
    "embed.weight.data = embed.weight.data + 0.005 * embed.weight.grad.data\n",
    "embed.weight.grad.data *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.60586344]\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.91412065]\n",
      "[1.33308181]\n",
      "[0.17888107]\n",
      "[0.01642682]\n",
      "[0.00449219]\n",
      "[0.00116295]\n",
      "[0.00037631]\n",
      "[0.00011018]\n",
      "[3.4257201e-05]\n",
      "[1.03142691e-05]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "data = Tensor([[0,1],\n",
    "               [2,3],\n",
    "               [4,5],\n",
    "               [6,7],\n",
    "               [8,9],\n",
    "               [10,11]], \n",
    "              autograd=True)\n",
    "\n",
    "target = Tensor([[1],[1],[1],[0],[0],[0]], autograd=True)\n",
    "\n",
    "embed = Embedding(12,5)\n",
    "weight = Tensor(np.random.rand(5,1), autograd=True)\n",
    "\n",
    "for i in range(10):\n",
    "    word_set = embed.weight.index_select(data)\n",
    "    bag = word_set.sum(1)\n",
    "    pred = Tensor.mm(bag,weight)\n",
    "    \n",
    "    loss = ((pred - target) * (pred - target)).sum(0)\n",
    "    \n",
    "    loss.backward(grad=None)\n",
    "    \n",
    "    weight.data = weight.data - 0.5 * weight.grad.data\n",
    "    weight.grad.data *= 0\n",
    "    \n",
    "    embed.weight.data = embed.weight.data - 0.05 * embed.weight.grad.data\n",
    "    embed.weight.grad.data *= 0\n",
    "    \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.91412065]\n",
      "[1.56928068]\n",
      "[0.78077574]\n",
      "[0.32692768]\n",
      "[0.11212161]\n",
      "[0.03230925]\n",
      "[0.008296]\n",
      "[0.002024]\n",
      "[0.00049514]\n",
      "[0.00012654]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "data = Tensor([[0,1],\n",
    "               [2,3],\n",
    "               [4,5],\n",
    "               [6,7],\n",
    "               [8,9],\n",
    "               [10,11]], \n",
    "              autograd=True)\n",
    "\n",
    "target = Tensor([[1],[1],[1],[0],[0],[0]], autograd=True)\n",
    "\n",
    "embed = Embedding(12,5)\n",
    "weight = Tensor(np.random.rand(5,1), autograd=True)\n",
    "\n",
    "for i in range(10):\n",
    "    word_set = embed.weight.index_select(data)\n",
    "    bag = word_set.sum(1)\n",
    "    pred = Tensor.mm(bag,weight)\n",
    "    \n",
    "    loss = ((pred - target) * (pred - target)).sum(0)\n",
    "    \n",
    "    loss.backward(grad=None)\n",
    "    \n",
    "    weight.data = weight.data - 0.05 * weight.grad.data\n",
    "    weight.grad.data *= 0\n",
    "    \n",
    "    embed.weight.data = embed.weight.data - 0.05 * embed.weight.grad.data\n",
    "    embed.weight.grad.data *= 0\n",
    "    \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
      "\n",
      "[0, 50181, 50184, 50710, 3099, 35875, 43044, 19511, 63549, 8254, 33862, 65607, 42062, 16462, 69713, 66646, 15463, 31337, 26733, 53368, 5760, 55427, 53382, 36491, 11919, 31887, 33937, 4241, 27283, 36508, 61088, 11937, 49319, 3754, 21682, 29362, 55989, 32442, 35002, 68800, 2244, 14022, 26311, 711, 38616, 70366, 19166, 28386, 18665, 16110, 19713, 36106, 58642, 53017, 13604, 35621, 4914, 73526, 15158, 18247, 37708, 18256, 58704, 38741, 30042, 58205, 56162, 69476, 34667, 47980, 7536, 46451, 13695, 41871, 7569, 43410, 14751, 20393, 53171, 38851, 68038, 57291, 71117, 52174, 59353, 22490, 36320, 34272, 3558, 23526, 64495, 38896, 60404, 37369]\n",
      "[[1], [0], [1], [0], [1], [0], [1], [0], [1], [0]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "f = open('reviews.txt')\n",
    "raw_reviews = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open('labels.txt')\n",
    "raw_labels = f.readlines()\n",
    "f.close()\n",
    "print(raw_reviews[0])\n",
    "\n",
    "# Se convierten los reviews en vectores de palabras\n",
    "tokens = list(map(lambda x: x.split(' '), raw_reviews))\n",
    "vocab = set()\n",
    "for oracion in tokens:\n",
    "    for palabra in oracion:\n",
    "        vocab.add(palabra)\n",
    "vocab = list(vocab)\n",
    "\n",
    "word2index = {}\n",
    "for i, palabra in enumerate(vocab):\n",
    "    word2index[palabra] = i\n",
    "\n",
    "# Generacion del Dataset de entrada\n",
    "input_dataset = list()\n",
    "for oracion in tokens:\n",
    "    oracion_indices = set()\n",
    "    for palabra in oracion:\n",
    "        try:\n",
    "            oracion_indices.add(word2index[palabra])\n",
    "        except:\n",
    "            ''\n",
    "    input_dataset.append(list(oracion_indices))\n",
    "print(input_dataset[0])\n",
    "\n",
    "# Generacion del dataset de salida\n",
    "target_dataset = list()\n",
    "for label in raw_labels:\n",
    "    if label == 'positive\\n':\n",
    "        target_dataset.append([1])\n",
    "    else:\n",
    "        target_dataset.append([0])   \n",
    "print(target_dataset[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# data = Tensor(input_dataset, autograd=True)\n",
    "target = Tensor(target_dataset, autograd=True)\n",
    "embed = Embedding(len(vocab), 100)\n",
    "linear = Tensor((np.random.randn(100, 1) * np.sqrt(2.0 / (100))), autograd=True)\n",
    "\n",
    "# for i in range(10):\n",
    "#     for j in range(len(target.data)):\n",
    "#         bag = embed.weight.index_select(input_dataset[j])\n",
    "bag1 = embed.weight.index_select(Tensor([input_dataset[3]])).sum(1)\n",
    "bag2 = embed.weight.index_select(Tensor(input_dataset[3])).sum(0)\n",
    "pred1 = bag1.mm(linear)\n",
    "pred2 = bag2.mm(linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.07938035]] [-0.07938035]\n"
     ]
    }
   ],
   "source": [
    "print(pred1, pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "test = Tensor(target_dataset[0], autograd=True)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.9[0.09899533]\n",
      "[array([0.14831021]), array([0.09899533])]\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "target = Tensor(target_dataset, autograd=True)\n",
    "embed = Embedding(len(vocab), 100)\n",
    "linear = Tensor((np.random.randn(100, 1) * np.sqrt(2.0 / (100))), autograd=True)\n",
    "\n",
    "loss_progression = list()\n",
    "for i in range(2):\n",
    "    samples = len(target.data) - 15000\n",
    "    acum_loss = 0\n",
    "    for j in range(samples):\n",
    "        bag = embed.weight.index_select(Tensor([input_dataset[j]])).sum(1)\n",
    "#         print('bag:',bag.data.shape)\n",
    "        pred = bag.mm(linear)\n",
    "#         print('pred:',pred.data.shape)\n",
    "        \n",
    "        target_j = Tensor([target_dataset[j]], autograd=True)\n",
    "#         print('target:', target_j.data.shape)\n",
    "        loss = ((pred - target_j) * (pred - target_j))\n",
    "#         print('loss:', loss.data.shape)\n",
    "        acum_loss = acum_loss + loss.data[0]\n",
    "        \n",
    "        loss.backward(grad=None)\n",
    "        linear.data = linear.data - (0.01 * linear.grad.data)\n",
    "        linear.grad.data *= 0\n",
    "        embed.weight.data = embed.weight.data - (0.01 * embed.weight.grad.data)\n",
    "        embed.weight.grad.data *= 0\n",
    "        \n",
    "        if(j % (samples/1000) == 0):\n",
    "            clear_output()\n",
    "#             sys.stdout.write('\\n')\n",
    "            sys.stdout.write(str((j/samples)*100))\n",
    "    loss_progression.append(acum_loss/samples)\n",
    "    print(loss_progression[i])\n",
    "print(loss_progression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obs.: El entrenamiento del modelo no es tan eficiente haciendo ```bag of words``` que si lo hicieramos con retropropagación manual\n",
    "\n",
    "En el caso de frameworks conocidos, como el caso de Pytorch, existen clases diseñadas para este tipo de tareas como el caso de [EmbeddingBag](https://pytorch.org/docs/stable/nn.html#embeddingbag) y [embedding_bag](https://pytorch.org/docs/stable/nn.html#embedding-bag)\n",
    "\n",
    "## Otras Referencias\n",
    "- [Logistic Regression Bag-of-Words classifier](https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html#example-logistic-regression-bag-of-words-classifier)\n",
    "- [Exercise: Computing Word Embeddings: Continuous Bag-of-Words](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#exercise-computing-word-embeddings-continuous-bag-of-words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
