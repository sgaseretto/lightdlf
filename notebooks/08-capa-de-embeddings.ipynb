{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings: \n",
    "> Una capa que traduce indices a activaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muchas veces es necesario pasarle a una red neuronal conjuntos de datos discretos, como categorías, elementos de grupos o conjuntos, un ejemplo de esto son las palabras en un texto. Como estas no pueden ser pasadas directamente a una red neuronal, es necesario convertirlas en una representacion matemática. \n",
    "\n",
    "Estas representaciones son llamados ```Embeddings``` que básicamente son vectores densos en donde un vector se corresoponde con una palabra, permitiendo que durante el entrenamiento de una red neuronal, esta capture información semántica de las palabras utilizando estas representaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder utilizar ```embeddings``` en una red neuronal debemos agregar soporte para la indexación de estos vectores a la clase ```Tensor```. Se debe asegurar que durante la retropropagación los gradientes sean puestos  en las mismas filas indexadas en la propagación hacia adelante. Esto se resuelve manteniendo los indices utilizados en el ```forward pass```. Para esto se agregan las siguientes lineas de código:\n",
    "\n",
    "El siguiente metodo agrega los indices al Tensor\n",
    "```\n",
    "def index_select(self, indices):\n",
    "    if(self.autograd):\n",
    "        new = Tensor(self.data[indices.data],\n",
    "                      autograd=True, \n",
    "                      creators=[self],\n",
    "                      creation_op='index_select')\n",
    "        # se agregan los indices como atributos del tensor\n",
    "        new.index_select_indices = indices\n",
    "        return new\n",
    "    return Tensor(self.data[indices.data])\n",
    "```\n",
    "Se agrega a ```backward()``` la logica para retropropagar el gradiente a los indices\n",
    "```\n",
    "if(self.creation_op=\"index_select\"):\n",
    "    new_grad = np.zeros_like(self.creators[0].data)\n",
    "    indices_ = self.index_select_indices.data.flatten()\n",
    "    grad_ = grad.data.reshape(len(indices_), -1)\n",
    "    for i in range(len(indices_)):\n",
    "        new_grad[indices_[i]] = new_grad[indices_[i]] + grad_[i]\n",
    "    self.creators[0].backward(Tensor(new_grad))\n",
    "```\n",
    "Esta condicion realiza lo siguiente:\n",
    "1. Inicializa un nuevo gradiente ```new_grad``` del tamaño correcto (el tamaño de la matriz original que fue indexada). \n",
    "2. Luego se aplanan los indices con ```flatten()``` para poder iterar sobre ellos. \n",
    "3. Colapsa el gradiente a una lista simple de filas ```grad_``` (la lista de indices en ```indices_``` y la lista de vectores ```grad_``` estarán en orden).\n",
    "4. Iterar sobre cada indice, agregarlo a la fila correcta del nuevo gradiente que estamos creando y ```retropropagar``` con ```backward()``` en el ```self.creator[0]```\n",
    "\n",
    "Adicionalmente, al agregar las siguientes lineas de código:\n",
    "```\n",
    "if (self.autograd):\n",
    "    if grad is None:\n",
    "        grad = Tensor(np.ones_like(self.data))\n",
    "```\n",
    "Es posible llamar a la función ```backward()``` sin pasar un gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Tensor(object):\n",
    "\n",
    "    def __init__(self, data,\n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        '''\n",
    "        Inicializa un tensor utilizando numpy\n",
    "\n",
    "        @data: una lista de numeros\n",
    "        @creators: lista de tensores que participarion en la creacion de un nuevo tensor\n",
    "        @creators_op: la operacion utilizada para combinar los tensores en el nuevo tensor\n",
    "        @autograd: determina si se realizara backprop o no sobre el tensor\n",
    "        @id: identificador del tensor, para poder dar seguimiento a los hijos y padres del mismo\n",
    "        '''\n",
    "        self.data = np.array(data)\n",
    "        self.creation_op = creation_op\n",
    "        self.creators = creators\n",
    "        self.grad = None\n",
    "        self.autograd = autograd\n",
    "        self.children = {}\n",
    "        # se asigna un id al tensor\n",
    "        if (id is None):\n",
    "            id = np.random.randint(0, 100000)\n",
    "        self.id = id\n",
    "\n",
    "        # se hace un seguimiento de cuantos hijos tiene un tensor\n",
    "        # si los creadores no es none\n",
    "        if (creators is not None):\n",
    "            # para cada tensor padre\n",
    "            for c in creators:\n",
    "                # se verifica si el tensor padre posee el id del tensor hijo\n",
    "                # en caso de no estar, agrega el id del tensor hijo al tensor padre\n",
    "                if (self.id not in c.children):\n",
    "                    c.children[self.id] = 1\n",
    "                # si el tensor ya se encuentra entre los hijos del padre\n",
    "                # y vuelve a aparece, se incrementa en uno\n",
    "                # la cantidad de apariciones del tensor hijo\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "\n",
    "    def all_children_grads_accounted_for(self):\n",
    "        '''\n",
    "        Verifica si un tensor ha recibido la cantidad\n",
    "        correcta de gradientes por cada uno de sus hijos\n",
    "        '''\n",
    "        # print('tensor id:', self.id)\n",
    "        for id, cnt in self.children.items():\n",
    "            if (cnt != 0):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def backward(self, grad, grad_origin=None):\n",
    "        '''\n",
    "        Funcion que propaga recursivamente el gradiente a los creators o padres del tensor\n",
    "\n",
    "        @grad: gradiente\n",
    "        @grad_orign\n",
    "        '''\n",
    "        if (self.autograd):\n",
    "            if grad is None:\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "            if (grad_origin is not None):\n",
    "                # Verifica para asegurar si se puede hacer retropropagacion\n",
    "                if (self.children[grad_origin.id] == 0):\n",
    "                    raise Exception(\"No se puede retropropagar mas de una vez\")\n",
    "                # o si se está esperando un gradiente, en dicho caso se decrementa\n",
    "                else:\n",
    "                    # el contador para ese hijo\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "\n",
    "        # acumula el gradiente de multiples hijos\n",
    "        if (self.grad is None):\n",
    "            self.grad = grad\n",
    "        else:\n",
    "            self.grad += grad\n",
    "\n",
    "        if (self.creators is not None and\n",
    "                (self.all_children_grads_accounted_for() or grad_origin is None)):\n",
    "\n",
    "            if (self.creation_op == 'neg'):\n",
    "                self.creators[0].backward(self.grad.__neg__())\n",
    "\n",
    "            if (self.creation_op == 'add'):\n",
    "                # al recibir self.grad, empieza a realizar backprop\n",
    "                self.creators[0].backward(self.grad, grad_origin=self)\n",
    "                self.creators[1].backward(self.grad, grad_origin=self)\n",
    "\n",
    "            if (self.creation_op == \"sub\"):\n",
    "                self.creators[0].backward(Tensor(self.grad.data), self)\n",
    "                self.creators[1].backward(Tensor(self.grad.__neg__().data), self)\n",
    "\n",
    "            if (self.creation_op == \"mul\"):\n",
    "                new = self.grad * self.creators[1]\n",
    "                self.creators[0].backward(new, self)\n",
    "                new = self.grad * self.creators[0]\n",
    "                self.creators[1].backward(new, self)\n",
    "\n",
    "            if (self.creation_op == \"mm\"):\n",
    "                layer = self.creators[0]  # activaciones => layer\n",
    "                weights = self.creators[1]  # pesos = weights\n",
    "                # c0 = self.creators[0]                       # activaciones => layer\n",
    "                # c1 = self.creators[1]                       # pesos = weights\n",
    "                # new = self.grad.mm(c1.transpose())  # grad = delta => delta x weights.T\n",
    "                new = Tensor.mm(self.grad, weights.transpose())  # grad = delta => delta x weights.T\n",
    "                layer.backward(new)\n",
    "                # c0.backward(new)\n",
    "                # new = self.grad.transpose().mm(c0).transpose() # (delta.T x layer).T = layer.T x delta\n",
    "                new = Tensor.mm(layer.transpose(), self.grad)  # layer.T x delta\n",
    "                weights.backward(new)\n",
    "                # c1.backward(new)\n",
    "\n",
    "            if (self.creation_op == \"transpose\"):\n",
    "                self.creators[0].backward(self.grad.transpose())\n",
    "\n",
    "            if (\"sum\" in self.creation_op):\n",
    "                dim = int(self.creation_op.split(\"_\")[1])\n",
    "                self.creators[0].backward(self.grad.expand(dim, self.creators[0].data.shape[dim]))\n",
    "\n",
    "            if (\"expand\" in self.creation_op):\n",
    "                dim = int(self.creation_op.split(\"_\")[1])\n",
    "                self.creators[0].backward(self.grad.sum(dim))\n",
    "                \n",
    "            if(self.creation_op == \"index_select\"):\n",
    "                # new_grad es un vector de 0s que luego contendra \n",
    "                # los gradientes para cada embedding\n",
    "                new_grad = np.zeros_like(self.creators[0].data)\n",
    "                # se obtienen los indices en un vector unidimensional\n",
    "                indices_ = self.index_select_indices.data.flatten()\n",
    "                grad_ = grad.data.reshape(len(indices_), -1)\n",
    "                for i in range(len(indices_)):\n",
    "                    new_grad[indices_[i]] = new_grad[indices_[i]] + grad_[i]\n",
    "                self.creators[0].backward(Tensor(new_grad))\n",
    "            \n",
    "            if (self.creation_op == \"sigmoid\"):\n",
    "                ones = Tensor(np.ones_like(self.grad.data))\n",
    "                self.creators[0].backward(self.grad * (self * (ones - self)))\n",
    "                \n",
    "            if (self.creation_op == \"tanh\"):\n",
    "                ones = Tensor(np.ones_like(self.grad.data))\n",
    "                self.creators[0].backward(self.grad * (ones - (self * self)))\n",
    "                \n",
    "            if (self.creation_op == 'relu'):\n",
    "                mask = Tensor(self.data > 0)\n",
    "                self.creators[0].backward(self.grad * mask)\n",
    "\n",
    "    def __neg__(self):\n",
    "        if (self.autograd):\n",
    "            return Tensor(self.data * -1,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op='neg')\n",
    "        return Tensor(self.data * -1)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        '''\n",
    "        @other: un Tensor\n",
    "        '''\n",
    "        if (self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self, other],\n",
    "                          creation_op='add')\n",
    "        return Tensor(self.data + other.data)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        '''\n",
    "        @other: un Tensor\n",
    "        '''\n",
    "        if (self.autograd and other.autograd):\n",
    "            return Tensor(self.data - other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self, other],\n",
    "                          creation_op='sub')\n",
    "        return Tensor(self.data - other.data)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        '''\n",
    "        @other: un Tensor\n",
    "        '''\n",
    "        if (self.autograd and other.autograd):\n",
    "            return Tensor(self.data * other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self, other],\n",
    "                          creation_op=\"mul\")\n",
    "        return Tensor(self.data * other.data)\n",
    "\n",
    "    def sum(self, dim):\n",
    "        '''\n",
    "        Suma atravez de dimensiones, si tenemos una matriz 2x3 y\n",
    "        aplicamos sum(0) sumara todos los valores de las filas\n",
    "        dando como resultado un vector 1x3, en cambio si se aplica\n",
    "        sum(1) el resultado es un vector 2x1\n",
    "\n",
    "        @dim: dimension para la suma\n",
    "        '''\n",
    "        if (self.autograd):\n",
    "            return Tensor(self.data.sum(dim),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sum_\" + str(dim))\n",
    "        return Tensor(self.data.sum(dim))\n",
    "\n",
    "    def expand(self, dim, copies):\n",
    "        '''\n",
    "        Se utiliza para retropropagar a traves de una suma sum().\n",
    "        Copia datos a lo largo de una dimension\n",
    "        '''\n",
    "\n",
    "        trans_cmd = list(range(0, len(self.data.shape)))\n",
    "        trans_cmd.insert(dim, len(self.data.shape))\n",
    "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)\n",
    "\n",
    "        if (self.autograd):\n",
    "            return Tensor(new_data,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"expand_\" + str(dim))\n",
    "        return Tensor(new_data)\n",
    "\n",
    "    def transpose(self):\n",
    "        if (self.autograd):\n",
    "            return Tensor(self.data.transpose(),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"transpose\")\n",
    "\n",
    "        return Tensor(self.data.transpose())\n",
    "\n",
    "    def mm(self, x):\n",
    "        if (self.autograd):\n",
    "            return Tensor(self.data.dot(x.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self, x],\n",
    "                          creation_op=\"mm\")\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "    \n",
    "    def index_select(self, indices):\n",
    "        if(self.autograd):\n",
    "            new = Tensor(self.data[indices.data],\n",
    "                          autograd=True, \n",
    "                          creators=[self],\n",
    "                          creation_op='index_select')\n",
    "            # se agregan los indices como atributos del tensor\n",
    "            new.index_select_indices = indices\n",
    "            return new\n",
    "        return Tensor(self.data[indices.data])\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        if (self.autograd):\n",
    "            return Tensor(1/(1+np.exp(-self.data)),\n",
    "                          autograd=True,\n",
    "                          creators=[self],creation_op='sigmoid')\n",
    "        return Tensor(1/(1+np.exp(-self.data)))\n",
    "    \n",
    "    def tanh(self):\n",
    "        if (self.autograd):\n",
    "            return Tensor(np.tanh(self.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op='tanh')\n",
    "        return Tensor(np.tanh(self.data))\n",
    "    \n",
    "    def relu(self):\n",
    "        ones_and_zeros = self.data > 0\n",
    "        if (self.autograd):\n",
    "            return Tensor(self.data * ones_and_zeros, \n",
    "                          autograd=True, \n",
    "                          creators=[self], \n",
    "                          creation_op='relu')\n",
    "        return Tensor(self.data * ones_and_zeros)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuicion detras de los ```Embeddings``` de ```bag of words```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Tensor([[1,0,0,0],     # estoy\n",
    "            [0,1,0,0],     # mal\n",
    "            [0,0,1,0],     # bien\n",
    "            [0,0,0,1]],    # normal\n",
    "           autograd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 0 0 0]\n",
      "  [0 0 1 0]]]\n"
     ]
    }
   ],
   "source": [
    "y = x.index_select(indices=Tensor([[0,2]]))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "z = y.sum(1)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward(Tensor([1,1,1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor z:\n",
      " [[1 0 1 0]] \n",
      " sum_1 \n",
      " [1 1 1 1] \n",
      "\n",
      "Tensor y:\n",
      " [[[1 0 0 0]\n",
      "  [0 0 1 0]]] \n",
      " index_select \n",
      " [[1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]] \n",
      "\n",
      "Tensor x:\n",
      " [[1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]] \n",
      " None \n",
      " [[1 1 1 1]\n",
      " [0 0 0 0]\n",
      " [1 1 1 1]\n",
      " [0 0 0 0]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Tensor z:\\n',z,'\\n',z.creation_op, '\\n', z.grad, '\\n')\n",
    "\n",
    "print('Tensor y:\\n',y,'\\n',y.creation_op, '\\n', y.grad, '\\n')\n",
    "\n",
    "print('Tensor x:\\n',x,'\\n',x.creation_op, '\\n', x.grad, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]]\n",
      "[0 2]\n",
      "[[1 1 1 1]\n",
      " [1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# Lo que ocurre internamente en el Tensor y\n",
    "aux = np.array([[1,1],[1,1],[1,1],[1,1]])\n",
    "indices = np.array([0,2]).flatten()\n",
    "grad = aux.reshape(len(indices), -1)\n",
    "print(aux)\n",
    "print(indices)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.parameters = list()\n",
    "\n",
    "    def get_parameters(self):\n",
    "        return self.parameters\n",
    "\n",
    "\n",
    "class Linear(Layer):\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super().__init__()\n",
    "        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0 / (n_inputs))\n",
    "        self.weight = Tensor(W, autograd=True)\n",
    "        self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
    "\n",
    "        self.parameters.append(self.weight)\n",
    "        self.parameters.append(self.bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return Tensor.mm(input, self.weight) + self.bias.expand(0, len(input.data))\n",
    "\n",
    "\n",
    "class Tanh(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.tanh()\n",
    "\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.sigmoid()\n",
    "\n",
    "\n",
    "class Relu(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.relu()\n",
    "\n",
    "\n",
    "class Sequential(Layer):\n",
    "\n",
    "    def __init__(self, layers=list()):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "\n",
    "    def get_parameters(self):\n",
    "        params = list()\n",
    "        for l in self.layers:\n",
    "            params += l.get_parameters()\n",
    "        return params\n",
    "\n",
    "\n",
    "class MSELoss(Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        return ((pred - target) * (pred - target)).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(object):\n",
    "\n",
    "    def __init__(self, parameters, alpha=0.1):\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def zero(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.data *= 0\n",
    "\n",
    "    def step(self, zero=True):\n",
    "        for p in self.parameters:\n",
    "            p.data = p.data - (self.alpha * p.grad.data)\n",
    "\n",
    "            if(zero):\n",
    "                p.grad.data *= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definiendo una capa de Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(Layer):\n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim = dim\n",
    "        # Esta inicializacion randomica es la convencion \n",
    "        # para inicializar embeddings de word2vec\n",
    "        weight = Tensor((np.random.rand(vocab_size, dim) - 0.5) / dim, autograd=True)\n",
    "        self.weight = weight\n",
    "        # se agregan los pesos a los parametros de la capa\n",
    "        self.parameters.append(self.weight)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.weight.index_select(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba de la capa de Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98874126]\n",
      "[0.6658868]\n",
      "[0.45639889]\n",
      "[0.31608168]\n",
      "[0.2260925]\n",
      "[0.16877423]\n",
      "[0.13120515]\n",
      "[0.10555487]\n",
      "[0.08731868]\n",
      "[0.07387834]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "data = Tensor(np.array([1,2,1,2]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "\n",
    "embed = Embedding(5,3)\n",
    "model = Sequential([embed, Tanh(), Linear(3,1), Sigmoid()])\n",
    "criterion = MSELoss()\n",
    "optim = SGD(model.get_parameters(), alpha=0.5)\n",
    "\n",
    "for i in range(10):\n",
    "    # Predecir\n",
    "    pred = model.forward(data)\n",
    "    \n",
    "    # Comparar\n",
    "    loss = criterion.forward(pred, target)\n",
    "    \n",
    "    # Aprender\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "    print(loss)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
