{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop Manual vs Autograd\n",
    "Ya implementado autograd con las funciones básicas de tensores probemos comparemos el codigo de una red neuronal lineal simple implementando manualmente ```backprop``` para la misma y utilizando la nueva clase ```Tensor```que implementamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.06643999]\n",
      "[0.49599078]\n",
      "[0.41806719]\n",
      "[0.35298133]\n",
      "[0.29725496]\n",
      "[0.2492326]\n",
      "[0.20785392]\n",
      "[0.17231261]\n",
      "[0.14193745]\n",
      "[0.1161398]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "data = np.array([[0,0],[0,1],[1,0],[1,1]])              # (4,2)\n",
    "target = np.array([[0],[1],[0],[1]])                    # (4,1)\n",
    "\n",
    "weights_0_1 = np.random.rand(2,3)                       # (2,3)\n",
    "weights_1_2 = np.random.rand(3,1)                       # (3,1)\n",
    "alpha = 0.1\n",
    "\n",
    "for i in range(10):\n",
    "    # Forward prop\n",
    "    layer_1 = np.dot(data, weights_0_1)                 # (4,3)\n",
    "    layer_2 = np.dot(layer_1, weights_1_2)              # (4,1)\n",
    "    \n",
    "    diff = layer_2 - target                             # (4,1)\n",
    "    loss = (diff * diff).sum(0)\n",
    "    \n",
    "    # Backprop\n",
    "    layer_1_grad = np.dot(diff, weights_1_2.T)          # (4,3)\n",
    "    weight_1_2_update = np.dot(layer_1.T, diff)         # (3,1)\n",
    "    weight_0_1_update = np.dot(data.T, layer_1_grad)    # (2,3)\n",
    "    \n",
    "    weights_1_2 = weights_1_2 - alpha * weight_1_2_update \n",
    "    weights_0_1 = weights_0_1 - alpha * weight_0_1_update\n",
    "    \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop usando ```autograd```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.58128304]\n",
      "[0.48988149]\n",
      "[0.41375111]\n",
      "[0.34489412]\n",
      "[0.28210124]\n",
      "[0.2254484]\n",
      "[0.17538853]\n",
      "[0.1324231]\n",
      "[0.09682769]\n",
      "[0.06849361]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from lightdlf.cpu.core import Tensor\n",
    "np.random.seed(0)\n",
    "\n",
    "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)   # (4,2)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)         # (4,1)\n",
    "\n",
    "w = list()\n",
    "w.append(Tensor(np.random.rand(2,3), autograd=True))                # (2,3)\n",
    "w.append(Tensor(np.random.rand(3,1), autograd=True))                # (3,1)\n",
    "alpha = 0.1\n",
    "\n",
    "for i in range(10):\n",
    "    pred = data.mm(w[0]).mm(w[1])                       # prediccion\n",
    "    \n",
    "    loss = ((pred - target) * (pred - target)).sum(0)   # funcion de perdida o loss function\n",
    "    \n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))      # retropropagacion de gradiente \n",
    "    for w_ in w:                                        # aprendizaje\n",
    "        w_.data = w_.data - (alpha * w_.grad.data)\n",
    "        w_.grad.data *= 0\n",
    "        \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ademas de haber hecho retropropagacion automáticamente, también se agregó la abstraccion de agregar todos los pesos a una lista, esto permite iterar por todos los pesos más facilmente para efectuar las actualizaciones de los pesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
