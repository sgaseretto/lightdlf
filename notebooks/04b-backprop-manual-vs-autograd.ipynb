{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop Manual vs Autograd\n",
    "Ya implementado autograd con las funciones básicas de tensores probemos comparemos el codigo de una red neuronal lineal simple implementando manualmente ```backprop``` para la misma y utilizando la nueva clase ```Tensor```que implementamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "tensor_1 = np.array([[0.5488135,0.71518937,0.60276338],\n",
    "                     [0.54488318,0.4236548,0.64589411]])\n",
    "tensor_2 = np.array([[0.43758721],\n",
    "                     [0.891773  ],\n",
    "                     [0.96366276]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.06644]\n",
      "[0.49599078]\n",
      "[0.41806719]\n",
      "[0.35298133]\n",
      "[0.29725497]\n",
      "[0.24923261]\n",
      "[0.20785392]\n",
      "[0.17231261]\n",
      "[0.14193745]\n",
      "[0.1161398]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "data = np.array([[0,0],[0,1],[1,0],[1,1]])              # (4,2)\n",
    "target = np.array([[0],[1],[0],[1]])                    # (4,1)\n",
    "\n",
    "# weights_0_1 = np.random.rand(2,3)                       # (2,3)\n",
    "# weights_1_2 = np.random.rand(3,1)                       # (3,1)\n",
    "weights_0_1 = copy.deepcopy(tensor_1)                   # (2,3)\n",
    "weights_1_2 = copy.deepcopy(tensor_2)                   # (3,1)\n",
    "alpha = 0.1\n",
    "\n",
    "for i in range(10):\n",
    "    # Forward prop\n",
    "    layer_1 = np.dot(data, weights_0_1)                 # (4,3)\n",
    "    layer_2 = np.dot(layer_1, weights_1_2)              # (4,1)\n",
    "    \n",
    "    diff = layer_2 - target                             # (4,1)\n",
    "    loss = (diff * diff).sum(0)\n",
    "    \n",
    "    # Backprop\n",
    "    layer_1_grad = np.dot(diff, weights_1_2.T)          # (4,3)\n",
    "    weight_1_2_update = np.dot(layer_1.T, diff)         # (3,1)\n",
    "    weight_0_1_update = np.dot(data.T, layer_1_grad)    # (2,3)\n",
    "    \n",
    "    weights_1_2 = weights_1_2 - alpha * weight_1_2_update \n",
    "    weights_0_1 = weights_0_1 - alpha * weight_0_1_update\n",
    "    \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop usando ```autograd```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.06644]\n",
      "[1.72520804]\n",
      "[0.97072979]\n",
      "[0.44845782]\n",
      "[0.19705059]\n",
      "[0.11889682]\n",
      "[0.0785371]\n",
      "[0.05072462]\n",
      "[0.03190535]\n",
      "[0.01958509]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from lightdlf_old.cpu.core import Tensor\n",
    "np.random.seed(0)\n",
    "\n",
    "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)   # (4,2)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)         # (4,1)\n",
    "\n",
    "w = list()\n",
    "# w.append(Tensor(np.random.rand(2,3), autograd=True))                # (2,3)\n",
    "# w.append(Tensor(np.random.rand(3,1), autograd=True))                # (3,1)\n",
    "w.append(Tensor(copy.deepcopy(tensor_1), autograd=True))            # (2,3)\n",
    "w.append(Tensor(copy.deepcopy(tensor_2), autograd=True))            # (3,1)\n",
    "alpha = 0.1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    pred = data.mm(w[0]).mm(w[1])                       # prediccion\n",
    "#     print(pred)\n",
    "    \n",
    "    loss = ((pred - target) * (pred - target)).sum(0)   # funcion de perdida o loss function\n",
    "    \n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))      # retropropagacion de gradiente \n",
    "    for w_ in w:                                        # aprendizaje\n",
    "        w_.data = w_.data - (alpha * w_.grad.data)\n",
    "        w_.grad.data *= 0\n",
    "        \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ademas de haber hecho retropropagacion automáticamente, también se agregó la abstraccion de agregar todos los pesos a una lista, esto permite iterar por todos los pesos más facilmente para efectuar las actualizaciones de los pesos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
