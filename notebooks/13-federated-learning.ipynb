{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a Federated Learning o Aprendizaje Federado\n",
    "\n",
    "## Problemas de privacidad al usar Deep Learning y otras tecnicas de Machine Learning\n",
    "\n",
    "Es sabido que las redes neuronales o ```deep learning``` como se lo conoce ahora es un sub-area del campo de ```Machine Learning```. Todo este grupo de algoritmos se caracterisa del resto de las otras áreas de la ```Inteligencia Artificial``` en que hecho de que su principal caracteristica es la capacidad que tienen de aprender utilizando datos, en lugar de usar reglas predefinidas. Pero muchas veces, los datos sobre los que se quiere crear un modelo de machine learning son datos muy personales y privados. Los mejores y más útiles modelos interactúan con los datos más personales de las personas y decirnos cosas sobre nosotros que hubiesen sido dificiles de saber de otra manera. Pero al mismo tiempo dar toda esta información requiere que confiemos en quien va a almacenar estos datos y que los cuidará para protejer nuestra privacidad, lo cual no siempre ocurre. Ejemplos de esto son:\n",
    "- **Aplicaciones en Medicina**: machine learning puede ayudar a mejorar dramáticamente diagnostico de enfermedades, como detección de tumores en imagenes de MRI, detectar con tiempo retinopatía diabética en imagenes de retina, detección de cancer en imagenes de melanoma, entre varias otras aplicaciónes más. Pero este tipo de datos son bastante sensibles ya que son datos de los pacientes, una filtración de este tipo de información sería muy grave.\n",
    "- **Recomendaciones**: ya sea recomendacion de productos, contenido o publicidad, estos modelos buscan personalizar la interacción de los usuarios en los servicios que están utilizando. Mientras más información personal del usuario sea posible de obtener para el modelo de recomendación, mucho mejor será la experiencia del usuario final, que recibirá recomendaciones más significativas. En el 2018 se reveló que una empresa de Cambridge utilizó datos personales de varios usuarios de Facebook para crear un perfil psicológico de cada uno y poder crear campañas de desinformación a través de facebook, que recomendaba anunciós con discursos de odio, con  para influenciar campañas electorales en el 2016 en Estados Unidos, influenciar la salida de Inglaterra de la EU (Brexit) entre varios otros escandalos.\n",
    "    - [Facebook–Cambridge Analytica data scandal](https://www.wikiwand.com/en/Facebook%E2%80%93Cambridge_Analytica_data_scandal)\n",
    "- **Credit Scoring**: modelos para saber que tan buenos pagadores de prestamos somos. Pueden utilizar informacion personal como historial crediticio, gastos varios y datos demograficos. Esta es información sensible que no querriamos que corra el riesgo de ser revelada a personas mal intencionadas. Por ejemplo, en el 2017 se reveló que Equifax ,una de las más grandes empresas que otorga credit scorings, entre varios otros servicios utilizando información personal de millones de personas, tuvo un breach enorme de información sensible de millones de personas.\n",
    "    - [Equifax Security Failing](https://www.wikiwand.com/en/Equifax#/Security_failings)\n",
    "    - [Equifax Breach: What Happened](https://www.csoonline.com/article/3444488/equifax-data-breach-faq-what-happened-who-was-affected-what-was-the-impact.html)\n",
    "    \n",
    "Ya que los datos son el recurso primordial para modelos como las redes neuronales, y los casos de uso más significativos de los mismos requiere que interactúen con datos personales, es necesario encontrar una manera de acceder a los mismos sin correr el riesgo de violar la privacidad de las personas.\n",
    "\n",
    "> Que pasaría si en lugar de acumular datos privados en un lugar centralizado para entrenar un modelo de deep learning, pudieramos enviar el modelo a donde se generan los datos y entrenar el modelo desde ahí, evitando así tener un solo punto de fallo desde el cual pueda ocurrir un ```breach``` de datos.\n",
    "\n",
    "Esto significa que:\n",
    "- Tecnicamente para poder participar en el entrenamiento de un modelo de deep learning, los usuarios no necesitan enviar sus datos a nadie, permitiendo así entrenar modelos valiosos con datos de salud, financieros, etc.\n",
    "- Personas y empresas que antes no podían compartir sus datos por cuestiones legales igual podrán generar valor gracias a ellos.\n",
    "\n",
    "## Federated Learning\n",
    "\n",
    "La premisa federated learning es que multiples datasets contienen información que es útil para resolver un problema, pero es dificil poder acceder a estos datasets en cantidades lo suficientemente grandes como para entrenar un modelo de deep learning que generalice lo suficientemente bien.\n",
    "\n",
    "Si bien el dataset puede tener suficiente informacion para entrenar un modelo de deep learning, la principal preocupación es que este también pueda contener información que no tenga relación con el aprendizaje del modelo, pero que pueda causar daños a alguien si es revelada. \n",
    "\n",
    "```Federated Learning``` se trata de enviar el modelo a un entorno seguro y aprender como resolver el problema sin la necesidad de mover los datos a ninguna parte. En este notebook veremos un ejemplo simple de ```federated learning.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "- [Federated Learning: Collaborative Machine Learning without Centralized Training Data](https://ai.googleblog.com/2017/04/federated-learning-collaborative.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "import sys\n",
    "import codecs\n",
    "\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso de Ejemplo: Detección de SPAM\n",
    "### Digamos que queremos entrenar un modelo para detectar spam entre los correos de varias personas\n",
    "Este caso de uso se trata de clasificar correos. Para esto vamos a usar un dataset de correos de ENRON, un dataset publico bastante conocido, por el escandalo generado por dicha empresa. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura y preprocesamiento del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cantidad de mails de spam: 9000\n",
      "Un correo de ejemplo:\n",
      " Subject: dobmeos with hgh my energy level has gone up ! stukm introducing doctor - formulated hgh human growth hormone - also called hgh is referred to in medical science as the master hormone . it is very plentiful when we are young , but near the age of twenty - one our bodies begin to produce less of it . by the time we are forty nearly everyone is deficient in hgh , and at eighty our production has normally diminished at least 90 - 95 % . advantages of hgh : - increased muscle strength - loss in body fat - increased bone density - lower blood pressure - quickens wound healing - reduces cellulite - improved vision - wrinkle disappearance - increased skin thickness texture - increased energy levels - improved sleep and emotional stability - improved memory and mental alertness - increased sexual potency - resistance to common illness - strengthened heart muscle - controlled cholesterol - controlled mood swings - new hair growth and color restore read more at this website unsubscribe \n",
      "\n",
      "cantidad de mails de ham: 22032\n",
      "Un correo de ejemplo:\n",
      " Subject: entex transistion the purpose of the email is to recap the kickoff meeting held on yesterday with members from commercial and volume managment concernig the entex account : effective january 2000 , thu nguyen ( x 37159 ) in the volume managment group , will take over the responsibility of allocating the entex contracts . howard and thu began some training this month and will continue to transition the account over the next few months . entex will be thu ' s primary account especially during these first few months as she learns the allocations process and the contracts . howard will continue with his lead responsibilites within the group and be available for questions or as a backup , if necessary ( thanks howard for all your hard work on the account this year ! ) . in the initial phases of this transistion , i would like to organize an entex \" account \" team . the team ( members from front office to back office ) would meet at some point in the month to discuss any issues relating to the scheduling , allocations , settlements , contracts , deals , etc . this hopefully will give each of you a chance to not only identify and resolve issues before the finalization process , but to learn from each other relative to your respective areas and allow the newcomers to get up to speed on the account as well . i would encourage everyone to attend these meetings initially as i believe this is a critical part to the success of the entex account . i will have my assistant to coordinate the initial meeting for early 1 / 2000 . if anyone has any questions or concerns , please feel free to call me or stop by . thanks in advance for everyone ' s cooperation . . . . . . . . . . . julie - please add thu to the confirmations distributions list \n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab, spam, ham = (set([\"<unk>\"]), list(), list())\n",
    "\n",
    "# Lecrura de spam\n",
    "# with codecs.open('datasets/enron-spam/spam.txt', 'r', encoding='utf-8', errors='ignore') as f:\n",
    "#     raw = f.readlines()\n",
    "f = codecs.open('datasets/enron-spam/spam.txt', 'r', encoding='utf-8', errors='ignore')\n",
    "raw = f.readlines()\n",
    "print('cantidad de mails de spam:', len(raw))\n",
    "print('Un correo de ejemplo:\\n',raw[0])\n",
    "# test = set(raw[0][:-2].split(\" \"))\n",
    "# print(test)\n",
    "for row in raw:\n",
    "    # se toma todas las palabras unicas de cada correo\n",
    "    spam.append(set(row[:-2].split(\" \")))\n",
    "    # por cada una de las palabras del ultimo correo \n",
    "    # agregado a la lista de spam\n",
    "    for word in spam[-1]:\n",
    "        # se agregan todas las palabras nuevas al vocabulario\n",
    "        vocab.add(word)\n",
    "\n",
    "# Repetimos el mismo proceso para el archivo ham.txt\n",
    "f = codecs.open('datasets/enron-spam/ham.txt', 'r', encoding='utf-8', errors='ignore')\n",
    "raw = f.readlines()\n",
    "print('cantidad de mails de ham:', len(raw))\n",
    "print('Un correo de ejemplo:\\n',raw[10])\n",
    "for row in raw:\n",
    "    ham.append(set(row[:-2].split(\" \")))\n",
    "    for word in ham[-1]:\n",
    "        vocab.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El codigo anterior es solo preprocesamiento. Lo preprocesamos para tenerlo listo a la hora de hacer forwardprop utilizando ```embeddings```. Algunas caracteristicas importantes del dataset preprocesado para poder entrenar el modelo son:\n",
    "- Todas las palabras son convertidas en una lista de indices\n",
    "- Todos los correos son convertidos en listas de 500 palabras exactamente, ya sea recortandolos o rellenandolos con el token ```<unk>```. Hacer esto hace que el dataset sea más fácil de procesar por el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomamos el vocabulario creado y creamos un diccionario\n",
    "# con las palabras y sus indices\n",
    "vocab, word2index = (list(vocab), {})\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "\n",
    "def to_indices(input, l=500):\n",
    "    indices = list()\n",
    "    for line in input:\n",
    "        # si la linea tiene menos palabras que l\n",
    "        if (len(line) < l):\n",
    "            # se completa la linea con el simbolo <unk> tantas\n",
    "            # veces hasta llegar a una longitud l\n",
    "            line = list(line) + (['<unk>'] * (l - len(line)))\n",
    "            idxs = list()\n",
    "            for word in line:\n",
    "                idxs.append(word2index[word])\n",
    "            indices.append(idxs)\n",
    "    return indices\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creacion de estructuras de datos a ser utilizadas para el entrenamiento de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se optienen los indices de spam y ham\n",
    "spam_idx = to_indices(spam)\n",
    "ham_idx = to_indices(ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupamos ham y spam en listas para crear\n",
    "# los conjuntos de prueba y entrenamiento\n",
    "train_spam_idx = spam_idx[0:-1000]\n",
    "train_ham_idx = ham_idx[0:-1000]\n",
    "test_spam_idx = spam_idx[-1000:]\n",
    "test_ham_idx = ham_idx[-1000:]\n",
    "\n",
    "# Creamos los conjuntos de test y entrenamiento\n",
    "train_data = list()\n",
    "train_target = list()\n",
    "\n",
    "test_data = list()\n",
    "test_target = list()\n",
    "\n",
    "for i in range(max(len(train_ham_idx), len(train_spam_idx))):\n",
    "    train_data.append(train_spam_idx[i%len(train_spam_idx)])\n",
    "    train_target.append([1])\n",
    "    \n",
    "    train_data.append(train_ham_idx[i%len(train_ham_idx)])\n",
    "    train_target.append([0])\n",
    "    \n",
    "for i in range(max(len(test_ham_idx), len(test_spam_idx))):\n",
    "    test_data.append(test_spam_idx[i%len(test_spam_idx)])\n",
    "    test_target.append([1])\n",
    "    \n",
    "    test_data.append(test_ham_idx[i%len(test_ham_idx)])\n",
    "    test_target.append([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definicion de las funciones para entrenar y testear el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightdlf.cpu.core import Tensor\n",
    "from lightdlf.cpu.layers import Embedding, MSELoss, CrossEntropyLoss\n",
    "from lightdlf.cpu.optimizers import SGD\n",
    "# from lightdlf.cpu.core2 import Tensor, Embedding, MSELoss, SGD\n",
    "\n",
    "def train(model, input_data, target_data, batch_size=500, iterations=5):\n",
    "    \n",
    "    criterion = MSELoss()\n",
    "    optim = SGD(parameters=model.get_parameters(), alpha=0.01)\n",
    "    \n",
    "    n_batches = int(len(input_data) / batch_size)\n",
    "    for iter in range(iterations):\n",
    "        iter_loss = 0\n",
    "        for b_i in range(n_batches):\n",
    "\n",
    "            # el token auxiliar <unk> se tiene que quedar en 0\n",
    "            # ya que no debe afectar al modelo\n",
    "            model.weight.data[word2index['<unk>']] *= 0 \n",
    "            input = Tensor(input_data[b_i*batch_size:(b_i+1)*batch_size], autograd=True)\n",
    "            target = Tensor(target_data[b_i*batch_size:(b_i+1)*batch_size], autograd=True)\n",
    "\n",
    "            pred = model.forward(input).sum(1).sigmoid()\n",
    "            loss = criterion.forward(pred,target)\n",
    "            # loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            iter_loss += loss.data[0] / batch_size\n",
    "\n",
    "            sys.stdout.write(\"\\r\\tLoss:\" + str(iter_loss / (b_i+1)))\n",
    "        print()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_input, test_output):\n",
    "    model.weight.data[word2index['<unk>']] *= 0\n",
    "    \n",
    "    input = Tensor(test_input, autograd=True)\n",
    "    target = Tensor(test_output, autograd=True)\n",
    "    \n",
    "    pred = model.forward(input).sum(1).sigmoid()\n",
    "    return ((pred.data > 0.5) == target.data).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de un modelo Centralizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Embedding(vocab_size=len(vocab), dim=1)\n",
    "model = Embedding(vocab_size=len(vocab), dim=2)\n",
    "# model.weight.data *= 0\n",
    "criterion = MSELoss()\n",
    "optim = SGD(parameters=model.get_parameters(), alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss:0.0431399306340297\n",
      "% Correct on Test Set: 98.275\n",
      "\tLoss:0.012896923366541803\n",
      "% Correct on Test Set: 99.0\n",
      "\tLoss:0.009016344772140578\n",
      "% Correct on Test Set: 99.25\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    model = train(model, train_data, train_target, iterations=1)\n",
    "    print(\"% Correct on Test Set: \" + str(test(model, test_data, test_target)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73603197, 0.81111928])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
