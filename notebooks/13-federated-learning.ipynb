{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a Federated Learning o Aprendizaje Federado\n",
    "\n",
    "## Problemas de privacidad al usar Deep Learning y otras tecnicas de Machine Learning\n",
    "\n",
    "Es sabido que las redes neuronales o ```deep learning``` como se lo conoce ahora es un sub-area del campo de ```Machine Learning```. Todo este grupo de algoritmos se caracterisa del resto de las otras áreas de la ```Inteligencia Artificial``` en que hecho de que su principal caracteristica es la capacidad que tienen de aprender utilizando datos, en lugar de usar reglas predefinidas. Pero muchas veces, los datos sobre los que se quiere crear un modelo de machine learning son datos muy personales y privados. Los mejores y más útiles modelos interactúan con los datos más personales de las personas y decirnos cosas sobre nosotros que hubiesen sido dificiles de saber de otra manera. Pero al mismo tiempo dar toda esta información requiere que confiemos en quien va a almacenar estos datos y que los cuidará para protejer nuestra privacidad, lo cual no siempre ocurre. Ejemplos de esto son:\n",
    "- **Aplicaciones en Medicina**: machine learning puede ayudar a mejorar dramáticamente diagnostico de enfermedades, como detección de tumores en imagenes de MRI, detectar con tiempo retinopatía diabética en imagenes de retina, detección de cancer en imagenes de melanoma, entre varias otras aplicaciónes más. Pero este tipo de datos son bastante sensibles ya que son datos de los pacientes, una filtración de este tipo de información sería muy grave.\n",
    "- **Recomendaciones**: ya sea recomendacion de productos, contenido o publicidad, estos modelos buscan personalizar la interacción de los usuarios en los servicios que están utilizando. Mientras más información personal del usuario sea posible de obtener para el modelo de recomendación, mucho mejor será la experiencia del usuario final, que recibirá recomendaciones más significativas. En el 2018 se reveló que una empresa de Cambridge utilizó datos personales de varios usuarios de Facebook para crear un perfil psicológico de cada uno y poder crear campañas de desinformación a través de facebook, que recomendaba anunciós con discursos de odio, con  para influenciar campañas electorales en el 2016 en Estados Unidos, influenciar la salida de Inglaterra de la EU (Brexit) entre varios otros escandalos.\n",
    "    - [Facebook–Cambridge Analytica data scandal](https://www.wikiwand.com/en/Facebook%E2%80%93Cambridge_Analytica_data_scandal)\n",
    "- **Credit Scoring**: modelos para saber que tan buenos pagadores de prestamos somos. Pueden utilizar informacion personal como historial crediticio, gastos varios y datos demograficos. Esta es información sensible que no querriamos que corra el riesgo de ser revelada a personas mal intencionadas. Por ejemplo, en el 2017 se reveló que Equifax ,una de las más grandes empresas que otorga credit scorings, entre varios otros servicios utilizando información personal de millones de personas, tuvo un breach enorme de información sensible de millones de personas.\n",
    "    - [Equifax Security Failing](https://www.wikiwand.com/en/Equifax#/Security_failings)\n",
    "    - [Equifax Breach: What Happened](https://www.csoonline.com/article/3444488/equifax-data-breach-faq-what-happened-who-was-affected-what-was-the-impact.html)\n",
    "    \n",
    "Ya que los datos son el recurso primordial para modelos como las redes neuronales, y los casos de uso más significativos de los mismos requiere que interactúen con datos personales, es necesario encontrar una manera de acceder a los mismos sin correr el riesgo de violar la privacidad de las personas.\n",
    "\n",
    "> Que pasaría si en lugar de acumular datos privados en un lugar centralizado para entrenar un modelo de deep learning, pudieramos enviar el modelo a donde se generan los datos y entrenar el modelo desde ahí, evitando así tener un solo punto de fallo desde el cual pueda ocurrir un ```breach``` de datos.\n",
    "\n",
    "Esto significa que:\n",
    "- Tecnicamente para poder participar en el entrenamiento de un modelo de deep learning, los usuarios no necesitan enviar sus datos a nadie, permitiendo así entrenar modelos valiosos con datos de salud, financieros, etc.\n",
    "- Personas y empresas que antes no podían compartir sus datos por cuestiones legales igual podrán generar valor gracias a ellos.\n",
    "\n",
    "## Federated Learning\n",
    "\n",
    "La premisa federated learning es que multiples datasets contienen información que es útil para resolver un problema, pero es dificil poder acceder a estos datasets en cantidades lo suficientemente grandes como para entrenar un modelo de deep learning que generalice lo suficientemente bien.\n",
    "\n",
    "Si bien el dataset puede tener suficiente informacion para entrenar un modelo de deep learning, la principal preocupación es que este también pueda contener información que no tenga relación con el aprendizaje del modelo, pero que pueda causar daños a alguien si es revelada. \n",
    "\n",
    "```Federated Learning``` se trata de enviar el modelo a un entorno seguro y aprender como resolver el problema sin la necesidad de mover los datos a ninguna parte. En este notebook veremos un ejemplo simple de ```federated learning.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "- [Federated Learning: Collaborative Machine Learning without Centralized Training Data](https://ai.googleblog.com/2017/04/federated-learning-collaborative.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "import sys\n",
    "import codecs\n",
    "\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso de Ejemplo: Detección de SPAM\n",
    "### Digamos que queremos entrenar un modelo para detectar spam entre los correos de varias personas\n",
    "Este caso de uso se trata de clasificar correos. Para esto vamos a usar un dataset de correos de ENRON, un dataset publico bastante conocido, por el escandalo generado por dicha empresa. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura y preprocesamiento del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cantidad de mails de spam: 9000\n",
      "Un correo de ejemplo:\n",
      " Subject: dobmeos with hgh my energy level has gone up ! stukm introducing doctor - formulated hgh human growth hormone - also called hgh is referred to in medical science as the master hormone . it is very plentiful when we are young , but near the age of twenty - one our bodies begin to produce less of it . by the time we are forty nearly everyone is deficient in hgh , and at eighty our production has normally diminished at least 90 - 95 % . advantages of hgh : - increased muscle strength - loss in body fat - increased bone density - lower blood pressure - quickens wound healing - reduces cellulite - improved vision - wrinkle disappearance - increased skin thickness texture - increased energy levels - improved sleep and emotional stability - improved memory and mental alertness - increased sexual potency - resistance to common illness - strengthened heart muscle - controlled cholesterol - controlled mood swings - new hair growth and color restore read more at this website unsubscribe \n",
      "\n",
      "cantidad de mails de ham: 22032\n",
      "Un correo de ejemplo:\n",
      " Subject: entex transistion the purpose of the email is to recap the kickoff meeting held on yesterday with members from commercial and volume managment concernig the entex account : effective january 2000 , thu nguyen ( x 37159 ) in the volume managment group , will take over the responsibility of allocating the entex contracts . howard and thu began some training this month and will continue to transition the account over the next few months . entex will be thu ' s primary account especially during these first few months as she learns the allocations process and the contracts . howard will continue with his lead responsibilites within the group and be available for questions or as a backup , if necessary ( thanks howard for all your hard work on the account this year ! ) . in the initial phases of this transistion , i would like to organize an entex \" account \" team . the team ( members from front office to back office ) would meet at some point in the month to discuss any issues relating to the scheduling , allocations , settlements , contracts , deals , etc . this hopefully will give each of you a chance to not only identify and resolve issues before the finalization process , but to learn from each other relative to your respective areas and allow the newcomers to get up to speed on the account as well . i would encourage everyone to attend these meetings initially as i believe this is a critical part to the success of the entex account . i will have my assistant to coordinate the initial meeting for early 1 / 2000 . if anyone has any questions or concerns , please feel free to call me or stop by . thanks in advance for everyone ' s cooperation . . . . . . . . . . . julie - please add thu to the confirmations distributions list \n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab, spam, ham = (set([\"<unk>\"]), list(), list())\n",
    "\n",
    "# Lecrura de spam\n",
    "# with codecs.open('datasets/enron-spam/spam.txt', 'r', encoding='utf-8', errors='ignore') as f:\n",
    "#     raw = f.readlines()\n",
    "f = codecs.open('datasets/enron-spam/spam.txt', 'r', encoding='utf-8', errors='ignore')\n",
    "raw = f.readlines()\n",
    "print('cantidad de mails de spam:', len(raw))\n",
    "print('Un correo de ejemplo:\\n',raw[0])\n",
    "# test = set(raw[0][:-2].split(\" \"))\n",
    "# print(test)\n",
    "for row in raw:\n",
    "    # se toma todas las palabras unicas de cada correo\n",
    "    spam.append(set(row[:-2].split(\" \")))\n",
    "    # por cada una de las palabras del ultimo correo \n",
    "    # agregado a la lista de spam\n",
    "    for word in spam[-1]:\n",
    "        # se agregan todas las palabras nuevas al vocabulario\n",
    "        vocab.add(word)\n",
    "\n",
    "# Repetimos el mismo proceso para el archivo ham.txt\n",
    "f = codecs.open('datasets/enron-spam/ham.txt', 'r', encoding='utf-8', errors='ignore')\n",
    "raw = f.readlines()\n",
    "print('cantidad de mails de ham:', len(raw))\n",
    "print('Un correo de ejemplo:\\n',raw[10])\n",
    "for row in raw:\n",
    "    ham.append(set(row[:-2].split(\" \")))\n",
    "    for word in ham[-1]:\n",
    "        vocab.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El codigo anterior es solo preprocesamiento. Lo preprocesamos para tenerlo listo a la hora de hacer forwardprop utilizando ```embeddings```. Algunas caracteristicas importantes del dataset preprocesado para poder entrenar el modelo son:\n",
    "- Todas las palabras son convertidas en una lista de indices\n",
    "- Todos los correos son convertidos en listas de 500 palabras exactamente, ya sea recortandolos o rellenandolos con el token ```<unk>```. Hacer esto hace que el dataset sea más fácil de procesar por el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomamos el vocabulario creado y creamos un diccionario\n",
    "# con las palabras y sus indices\n",
    "vocab, word2index = (list(vocab), {})\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "\n",
    "def to_indices(input, l=500):\n",
    "    indices = list()\n",
    "    for line in input:\n",
    "        # si la linea tiene menos palabras que l\n",
    "        if (len(line) < l):\n",
    "            # se completa la linea con el simbolo <unk> tantas\n",
    "            # veces hasta llegar a una longitud l\n",
    "            line = list(line) + (['<unk>'] * (l - len(line)))\n",
    "            idxs = list()\n",
    "            for word in line:\n",
    "                idxs.append(word2index[word])\n",
    "            indices.append(idxs)\n",
    "    return indices\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creacion de estructuras de datos a ser utilizadas para el entrenamiento de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se optienen los indices de spam y ham\n",
    "spam_idx = to_indices(spam)\n",
    "ham_idx = to_indices(ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupamos ham y spam en listas para crear\n",
    "# los conjuntos de prueba y entrenamiento\n",
    "train_spam_idx = spam_idx[0:-1000]\n",
    "train_ham_idx = ham_idx[0:-1000]\n",
    "test_spam_idx = spam_idx[-1000:]\n",
    "test_ham_idx = ham_idx[-1000:]\n",
    "\n",
    "# Creamos los conjuntos de test y entrenamiento\n",
    "train_data = list()\n",
    "train_target = list()\n",
    "\n",
    "test_data = list()\n",
    "test_target = list()\n",
    "\n",
    "for i in range(max(len(train_ham_idx), len(train_spam_idx))):\n",
    "    train_data.append(train_spam_idx[i%len(train_spam_idx)])\n",
    "    train_target.append([1])\n",
    "    \n",
    "    train_data.append(train_ham_idx[i%len(train_ham_idx)])\n",
    "    train_target.append([0])\n",
    "    \n",
    "for i in range(max(len(test_ham_idx), len(test_spam_idx))):\n",
    "    test_data.append(test_spam_idx[i%len(test_spam_idx)])\n",
    "    test_target.append([1])\n",
    "    \n",
    "    test_data.append(test_ham_idx[i%len(test_ham_idx)])\n",
    "    test_target.append([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definicion de las funciones para entrenar y testear el modelo\n",
    "Definimos las funciones que nos van a permitir inicializar, entrenar y evaluar el modelo centralizado de detección de spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightdlf.cpu.core import Tensor\n",
    "from lightdlf.cpu.layers import Embedding, MSELoss, CrossEntropyLoss\n",
    "from lightdlf.cpu.optimizers import SGD\n",
    "# from lightdlf.cpu.core2 import Tensor, Embedding, MSELoss, SGD\n",
    "\n",
    "def train(model, input_data, target_data, batch_size=500, iterations=5):\n",
    "    \n",
    "    criterion = MSELoss()\n",
    "    optim = SGD(parameters=model.get_parameters(), alpha=0.01)\n",
    "    \n",
    "    n_batches = int(len(input_data) / batch_size)\n",
    "    for iter in range(iterations):\n",
    "        iter_loss = 0\n",
    "        for b_i in range(n_batches):\n",
    "\n",
    "            # el token auxiliar <unk> se tiene que quedar en 0\n",
    "            # ya que no debe afectar al modelo\n",
    "            model.weight.data[word2index['<unk>']] *= 0 \n",
    "            input = Tensor(input_data[b_i*batch_size:(b_i+1)*batch_size], autograd=True)\n",
    "            target = Tensor(target_data[b_i*batch_size:(b_i+1)*batch_size], autograd=True)\n",
    "\n",
    "            pred = model.forward(input).sum(1).sigmoid()\n",
    "            loss = criterion.forward(pred,target)\n",
    "            # loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            iter_loss += loss.data[0] / batch_size\n",
    "\n",
    "            sys.stdout.write(\"\\r\\tLoss:\" + str(iter_loss / (b_i+1)))\n",
    "        print()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_input, test_output):\n",
    "    model.weight.data[word2index['<unk>']] *= 0\n",
    "    \n",
    "    input = Tensor(test_input, autograd=True)\n",
    "    target = Tensor(test_output, autograd=True)\n",
    "    \n",
    "    pred = model.forward(input).sum(1).sigmoid()\n",
    "    return ((pred.data > 0.5) == target.data).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de un modelo Centralizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Embedding(vocab_size=len(vocab), dim=2)\n",
    "model = Embedding(vocab_size=len(vocab), dim=1)\n",
    "model.weight.data *= 0\n",
    "criterion = MSELoss()\n",
    "optim = SGD(parameters=model.get_parameters(), alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss:0.03714041686087146\n",
      "% Correcto en el conjunto de entrenamiento: 98.65\n",
      "\tLoss:0.011258669226059108\n",
      "% Correcto en el conjunto de entrenamiento: 99.15\n",
      "\tLoss:0.008068268387986223\n",
      "% Correcto en el conjunto de entrenamiento: 99.45\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    model = train(model, train_data, train_target, iterations=1)\n",
    "    print(\"% Correcto en el conjunto de entrenamiento: \" + str(test(model, test_data, test_target)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de 3 iteraciones logramos entrenar un modelo que puede predecir correos de spam con una precision del ```99.45%```\n",
    "\n",
    "### Analisis de los embedings generados\n",
    "Hemos generado un modelo donde todos los ```embeddings``` tienen una dimension de 1, veamos los ```embeddings``` de palabras comunes en correos de spam y comunes en correos normales de una empresa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras comunes en correos de spam:\n",
      "\t- palabra: penis \n",
      "\tidx: 18533 ,\n",
      "\tembedding: [0.09662769] \n",
      "\n",
      "\t- palabra: viagra \n",
      "\tidx: 25087 ,\n",
      "\tembedding: [0.20160151] \n",
      "\n",
      "\t- palabra: spam \n",
      "\tidx: 36488 ,\n",
      "\tembedding: [0.11120405] \n",
      "\n",
      "Palabras comunes en correos normales\n",
      "\t- palabra: critical \n",
      "\tidx: 21757 ,\n",
      "\tembedding: [-0.02622459] \n",
      "\n",
      "\t- palabra: assistant \n",
      "\tidx: 482 ,\n",
      "\tembedding: [-0.02255735] \n",
      "\n",
      "\t- palabra: meetings \n",
      "\tidx: 41819 ,\n",
      "\tembedding: [-0.02443877] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Palabras comunes en correos de spam:')\n",
    "print('\\t- palabra: penis', '\\n\\tidx:', word2index['penis'], ',\\n\\tembedding:', model.weight.data[word2index['penis']], '\\n')\n",
    "print('\\t- palabra: viagra', '\\n\\tidx:', word2index['viagra'], ',\\n\\tembedding:', model.weight.data[word2index['viagra']], '\\n')\n",
    "print('\\t- palabra: spam', '\\n\\tidx:', word2index['spam'], ',\\n\\tembedding:', model.weight.data[word2index['spam']], '\\n')\n",
    "# print('- palabra: cocaine', '\\nidx:', word2index['cocaine'], ',\\nembedding:', model.weight.data[word2index['cocaine']], '\\n')\n",
    "\n",
    "print('Palabras comunes en correos normales')\n",
    "print('\\t- palabra: critical', '\\n\\tidx:', word2index['critical'], ',\\n\\tembedding:', model.weight.data[word2index['critical']], '\\n')\n",
    "print('\\t- palabra: assistant', '\\n\\tidx:', word2index['assistant'], ',\\n\\tembedding:', model.weight.data[word2index['assistant']], '\\n')\n",
    "print('\\t- palabra: meetings', '\\n\\tidx:', word2index['meetings'], ',\\n\\tembedding:', model.weight.data[word2index['meetings']], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que los embeddings de palabras comunes en correos de spam tienen un valor positivo, mientras que las palabras comunes en correos normales tienden a valores negativos, esto es porque estamos usando la función ```sigmoide``` para poder clasificar estos correos, donde:\n",
    "- 0 = todos los correos normales o ```ham```\n",
    "- 1 = todos los correos que son ```spam```\n",
    "\n",
    "En la función ```sigmoide```, los valores por debajo de 0 tienden a tendrán como valor ```0.5 o menos```, como nuestro modelo es un modelo conocido como ```bag of words```, si las palabras comunes en un correo de spam tienen un valor negativo, mientras, más de estas haya en un correo, estas sumarán un numero muy por debajo de 0, por lo que la función ```sigmoide``` tenderá a 0, esto se puede ver claramente en los embeddings de las palabras más arriba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated Learning: Volviendo el modelo Centralizado en uno Federado\n",
    "El ejemplo anterior es la forma tradicional de entrenar un modelo de machine learning en donde:\n",
    "1. Cada usuario envia sus datos a un servidor central\n",
    "2. El servidor central entrena un modelo global en base a los datos\n",
    "3. El modelo y los datos quedan en el servidor central\n",
    "\n",
    "Al tener todos los datos en un servidor central, tenemos el problema que habíamos menciondo, de el cliente pierde el control de sus datos y por ende de su privacidad. Un breach en el servidor central es suficiente para vulnerar la privacidad de miles de usuarios.\n",
    "\n",
    "Como habíamos mencionado, la solucion a este problema es utilizar federated learning. Para ello simulemos un entorno de entrenamiento donde tengamos usuarios con multiples colecciones diferentes de correos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob = (train_data[0:1000], train_target[0:1000])\n",
    "alice = (train_data[1000:2000], train_target[1000:2000])\n",
    "sue = (train_data[2000:], train_target[2000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cantidad de correos por usuario\n",
      "- bob 1000\n",
      "- alice 1000\n",
      "- sue 39908\n"
     ]
    }
   ],
   "source": [
    "print(\"cantidad de correos por usuario\")\n",
    "print('- bob',len(bob[0]))\n",
    "print('- alice',len(alice[0]))\n",
    "print('- sue',len(sue[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos estos tres datasets, podemos hacer el mismo entrenamiento que habíamos hecho anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Embedding(vocab_size=len(vocab), dim=1)\n",
    "model.weight.data *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando la ronda de entrenamiento...\n",
      "\tPaso 1: enviamos el modelo a Bob\n",
      "\tLoss:0.21908166249699718\n",
      "\n",
      "\tPaso 2: enviamos el modelo a Alice\n",
      "\tLoss:0.2937106899184867\n",
      "\n",
      "\tPaso 3: enviamos el modelo a Sue\n",
      "\tLoss:0.03333996697717589\n",
      "\n",
      "\tModelo promedio de todos los modelos\n",
      "\t% Correcto en el conjunto de entrenamiento: 84.05\n",
      "Iteramos\n",
      "\n",
      "Iniciando la ronda de entrenamiento...\n",
      "\tPaso 1: enviamos el modelo a Bob\n",
      "\tLoss:0.0662536748363041\n",
      "\n",
      "\tPaso 2: enviamos el modelo a Alice\n",
      "\tLoss:0.09595374225556821\n",
      "\n",
      "\tPaso 3: enviamos el modelo a Sue\n",
      "\tLoss:0.02029024788114074\n",
      "\n",
      "\tModelo promedio de todos los modelos\n",
      "\t% Correcto en el conjunto de entrenamiento: 92.25\n",
      "Iteramos\n",
      "\n",
      "Iniciando la ronda de entrenamiento...\n",
      "\tPaso 1: enviamos el modelo a Bob\n",
      "\tLoss:0.030819682914453833\n",
      "\n",
      "\tPaso 2: enviamos el modelo a Alice\n",
      "\tLoss:0.0358032489173609\n",
      "\n",
      "\tPaso 3: enviamos el modelo a Sue\n",
      "\tLoss:0.01536846160847025\n",
      "\n",
      "\tModelo promedio de todos los modelos\n",
      "\t% Correcto en el conjunto de entrenamiento: 98.8\n",
      "Iteramos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "for i in range(3):\n",
    "    # Tomamos el modelo que inicializamos y por cada set de datos\n",
    "    # Creamos una copia del modelo (deepcopy) y entrenamos\n",
    "    # un modelo por cada conjunto de datos\n",
    "    print('Iniciando la ronda de entrenamiento...')\n",
    "    print('\\tPaso 1: enviamos el modelo a Bob')\n",
    "    bob_model = train(copy.deepcopy(model), bob[0], bob[1], iterations=1)\n",
    "    \n",
    "    print('\\n\\tPaso 2: enviamos el modelo a Alice')\n",
    "    alice_model = train(copy.deepcopy(model), alice[0], alice[1], iterations=1)\n",
    "    \n",
    "    print('\\n\\tPaso 3: enviamos el modelo a Sue')\n",
    "    sue_model = train(copy.deepcopy(model), sue[0], sue[1], iterations=1)\n",
    "    \n",
    "    print('\\n\\tModelo promedio de todos los modelos')\n",
    "    model.weight.data = (bob_model.weight.data + \\\n",
    "                         alice_model.weight.data + \\\n",
    "                         sue_model.weight.data)/3\n",
    "    \n",
    "    print('\\t% Correcto en el conjunto de entrenamiento: ' + \\\n",
    "          str(test(model, test_data, test_target)*100))\n",
    "    print('Iteramos\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenando de esta manera obtenemos un modelo con casi el mismo rendimiento que el modelo centralizado, y en teoría no necesitamos tener acceso a los datos de entrenamiento para que cada usuario cambie el modelo de alguna manera. \n",
    "\n",
    "De esta manera, es posible descubrir algo de los datasets con los que se está entrenando? Que de alguna manera, durante el entrenamiento, se pueda descubrir algo del dataset de un usuario y así vulnerar la privacidad del mismo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vulnerando Federated Learning\n",
    "*Veamos un ejemplo en donde como es posible que un modelo memorice información del conjunto de entrenamiento y por ende, vulnerar la privacidad de un usuario.*\n",
    "\n",
    "Federated Learning tiene dos grandes desafíos:\n",
    "- Rendimiento o Performance\n",
    "- Privacidad\n",
    "\n",
    "Los cuales son más difíciles de manejar cuando cada usuario tiene un dataset de entrenamiento con muy pocos ejemplos. Si tenemos miles de ususarios, cada uno con muy pocos ejemplos pasa que:\n",
    "1. El modelo en lugar de generalizar, empieza a memorizar los datos utilizados para su entrenamiento.\n",
    "2. Se pasa más tiempo enviando y recibiendo el modelo de los usuarios y poco tiempo entrenando el modelo en sí.\n",
    "\n",
    "Miremos un ejemplo donde uno de los usuarios tiene muy pocos ejemplos de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss:0.25\n"
     ]
    }
   ],
   "source": [
    "bobs_email = [\"my\", \"computer\", \"password\", \"is\", \"pizza\"]\n",
    "\n",
    "bob_input = np.array([[word2index[x] for x in bobs_email]])\n",
    "bob_target = np.array([0])\n",
    "\n",
    "model = Embedding(vocab_size=len(vocab), dim=1)\n",
    "model.weight.data *= 0\n",
    "\n",
    "bobs_model = train(copy.deepcopy(model), \n",
    "                   bob_input, \n",
    "                   bob_target, \n",
    "                   iterations=1, \n",
    "                   batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo de bob, pero bob solo tenía un correo, y no solamente eso, dicho correo contenía información sensible sobre como acceder a su computadora. Ahora, lo que nosotros obtuvimos es un modelo, no los datos de bob. Aún así, es posible vulnerar la privacidad de bob?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is\n",
      "computer\n",
      "password\n",
      "pizza\n",
      "my\n"
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(model.weight.data - bobs_model.weight.data):\n",
    "    if (v != 0):\n",
    "        print(vocab[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y así como así, solo se necesitó saber como variaron los pesos al actualizar el modelo para poder descubrir la contraseña de la computadora de bob, violando así su privacidad.\n",
    "\n",
    "Que pasaría si pudieramos encriptar los modelos, realizar operaciones sobre el mismo y luego desencriptarlo para proteger la información?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cifrado homomórfico\n",
    "*Realizar operaciones aritmeticas sobre valores encriptados es posible*\n",
    "\n",
    "Básicamente poder realizar operaciones aritméticas sobre valores encriptados se llama cifrado homomorfico. Cifrado Homomorfico es toda un área de investigacion en sí misma, en este notebook nos vamos a centrar en la capacidad de realizar adiciones entre valores encriptados. Contamos con:\n",
    "- Una clave pública para encriptar los valores y\n",
    "- Una clave privada para desencriptar los valores\n",
    "\n",
    "Veamos un ejemplo de esto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En caso de ser la primera vez que se corre este notebook \n",
    "# y no se tiene la libreria phe, instalarla con la siguiente linea\n",
    "# https://github.com/n1analytics/python-paillier\n",
    "# !pip install phe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El valor de z es: 8\n"
     ]
    }
   ],
   "source": [
    "import phe\n",
    "public_key, private_key = phe.generate_paillier_keypair(n_length=1024)\n",
    "\n",
    "x = public_key.encrypt(5)\n",
    "y = public_key.encrypt(3)\n",
    "\n",
    "z = x + y\n",
    "\n",
    "z_plain = private_key.decrypt(z)\n",
    "print('El valor de z es:', z_plain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El valor de w es: 6\n",
      "El valor de w es: 10\n"
     ]
    }
   ],
   "source": [
    "# Otras operacioes posibles con encriptacion o cifrado homomorfico\n",
    "w = x + 1\n",
    "w_plain = private_key.decrypt(w)\n",
    "print('El valor de w es:', w_plain)\n",
    "w = x * 2\n",
    "w_plain = private_key.decrypt(w)\n",
    "print('El valor de w es:', w_plain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hagamos un ejemplo de lo que sería entrenar un modelo con encriptación homomorfica. Primero creemos una funcion que nos permita encriptar un modelo que hayamos entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_encrypt(model, input, target, pubkey):\n",
    "    # A fines demostrativos, esta funcion solo funciona \n",
    "    # para Embeddings con una sola dimension\n",
    "    new_model = train(copy.deepcopy(model), input, target, iterations=1)\n",
    "    \n",
    "    encrypt_weights = list()\n",
    "    for val in new_model.weight.data[:,0]:\n",
    "        encrypt_weights.append(public_key.encrypt(val))\n",
    "    ew = np.array(encrypt_weights).reshape(new_model.weight.data.shape)\n",
    "    \n",
    "    return ew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Embedding(vocab_size=len(vocab), dim=1)\n",
    "model.weight.data *= 0\n",
    "\n",
    "public_key, private_key = phe.generate_paillier_keypair(n_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando la Iteracion de entrenamiento...\n",
      "\tPaso 1: enviar modelo a Bob\n",
      "\tLoss:0.21908166249699718\n",
      "\n",
      "\tPaso 2: enviar modelo a Alice\n",
      "\tLoss:0.2937106899184867\n",
      "\n",
      "\tPaso 1: enviar modelo a Sue\n",
      "\tLoss:0.03333996697717589\n",
      "\n",
      "\tPaso 4: Bob, Alice y Sue envian\n",
      "\ty agregan sus modelos encriptados ente sí\n",
      "\n",
      "\tPaso 5: Solo el modelo agregado\n",
      "\tse envia devuelta al dueño del modelo\n",
      "\tque puede desencriptarlo\n",
      "\tCorrectos en el conjunto de prueba:84.05\n",
      "\n",
      "Iniciando la Iteracion de entrenamiento...\n",
      "\tPaso 1: enviar modelo a Bob\n",
      "\tLoss:0.06625367483630411\n",
      "\n",
      "\tPaso 2: enviar modelo a Alice\n",
      "\tLoss:0.09595374225556821\n",
      "\n",
      "\tPaso 1: enviar modelo a Sue\n",
      "\tLoss:0.02029024788114074\n",
      "\n",
      "\tPaso 4: Bob, Alice y Sue envian\n",
      "\ty agregan sus modelos encriptados ente sí\n",
      "\n",
      "\tPaso 5: Solo el modelo agregado\n",
      "\tse envia devuelta al dueño del modelo\n",
      "\tque puede desencriptarlo\n",
      "\tCorrectos en el conjunto de prueba:92.25\n",
      "\n",
      "Iniciando la Iteracion de entrenamiento...\n",
      "\tPaso 1: enviar modelo a Bob\n",
      "\tLoss:0.030819682914453833\n",
      "\n",
      "\tPaso 2: enviar modelo a Alice\n",
      "\tLoss:0.0358032489173609\n",
      "\n",
      "\tPaso 1: enviar modelo a Sue\n",
      "\tLoss:0.01536846160847025\n",
      "\n",
      "\tPaso 4: Bob, Alice y Sue envian\n",
      "\ty agregan sus modelos encriptados ente sí\n",
      "\n",
      "\tPaso 5: Solo el modelo agregado\n",
      "\tse envia devuelta al dueño del modelo\n",
      "\tque puede desencriptarlo\n",
      "\tCorrectos en el conjunto de prueba:98.8\n",
      "\n",
      "Iniciando la Iteracion de entrenamiento...\n",
      "\tPaso 1: enviar modelo a Bob\n",
      "\tLoss:0.01727558933300258\n",
      "\n",
      "\tPaso 2: enviar modelo a Alice\n",
      "\tLoss:0.018830500591261824\n",
      "\n",
      "\tPaso 1: enviar modelo a Sue\n",
      "\tLoss:0.01275228530278016\n",
      "\n",
      "\tPaso 4: Bob, Alice y Sue envian\n",
      "\ty agregan sus modelos encriptados ente sí\n",
      "\n",
      "\tPaso 5: Solo el modelo agregado\n",
      "\tse envia devuelta al dueño del modelo\n",
      "\tque puede desencriptarlo\n",
      "\tCorrectos en el conjunto de prueba:99.05000000000001\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print('\\nIniciando la Iteracion de entrenamiento...')\n",
    "    print('\\tPaso 1: enviar modelo a Bob')\n",
    "    bob_encrypted_model = train_and_encrypt(copy.deepcopy(model),\n",
    "                                            bob[0], bob[1], public_key)\n",
    "    print('\\n\\tPaso 2: enviar modelo a Alice')\n",
    "    alice_encrypted_model = train_and_encrypt(copy.deepcopy(model),\n",
    "                                           alice[0], alice[1], public_key)\n",
    "    print('\\n\\tPaso 1: enviar modelo a Sue')\n",
    "    sue_encrypted_model = train_and_encrypt(copy.deepcopy(model),\n",
    "                                           sue[0], sue[1], public_key)\n",
    "    print('\\n\\tPaso 4: Bob, Alice y Sue envian')\n",
    "    print('\\ty agregan sus modelos encriptados ente sí')\n",
    "    aggregated_model = bob_encrypted_model + \\\n",
    "                       alice_encrypted_model + \\\n",
    "                       sue_encrypted_model\n",
    "    \n",
    "    print('\\n\\tPaso 5: Solo el modelo agregado')\n",
    "    print('\\tse envia devuelta al dueño del modelo')\n",
    "    print('\\tque puede desencriptarlo')\n",
    "    raw_values = list()\n",
    "    for val in aggregated_model.flatten():\n",
    "        raw_values.append(private_key.decrypt(val))\n",
    "    model.weight.data = np.array(raw_values).reshape(model.weight.data.shape)/3\n",
    "    \n",
    "    print(\"\\tCorrectos en el conjunto de prueba:\" + \\\n",
    "          str(test(model, test_data, test_target) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
