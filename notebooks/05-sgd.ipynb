{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agregando Optimizacion Automática\n",
    "El framework hasta el momento ya es capaz de realizar la retropropagacion automáticamente, pero todavía no actualiza todos los pesos de forma automática, para definiremos la clase ```SGD``` que realiza ```Stochastic Gradient Descent``` o ```Descenso de gradiente Estocastico```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(object):\n",
    "\n",
    "    def __init__(self, parameters, alpha=0.1):\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def zero(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.data *= 0\n",
    "\n",
    "    def step(self, zero=True):\n",
    "        for p in self.parameters:\n",
    "            p.data = p.data - (self.alpha * p.grad.data)\n",
    "\n",
    "            if(zero):\n",
    "                p.grad.data *= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La misma red neuronal del [notebook](04b-backprop-manual-vs-autograd.ipynb) anterior, utilizando ```stochastic gradient descent``` para optimizarla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.58128304]\n",
      "[0.48988149]\n",
      "[0.41375111]\n",
      "[0.34489412]\n",
      "[0.28210124]\n",
      "[0.2254484]\n",
      "[0.17538853]\n",
      "[0.1324231]\n",
      "[0.09682769]\n",
      "[0.06849361]\n"
     ]
    }
   ],
   "source": [
    "from lightdlf_old.cpu.core import Tensor\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)   # (4,2)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)         # (4,1)\n",
    "\n",
    "w = list()\n",
    "w.append(Tensor(np.random.rand(2,3), autograd=True))                # (2,3)\n",
    "w.append(Tensor(np.random.rand(3,1), autograd=True))                # (3,1)\n",
    "optim = SGD(w, alpha=0.1)\n",
    "\n",
    "for i in range(10):\n",
    "    pred = data.mm(w[0]).mm(w[1])\n",
    "    \n",
    "    loss = ((pred - target) * (pred - target)).sum(0)\n",
    "    \n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "    \n",
    "    print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
